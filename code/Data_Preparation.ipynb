{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T00:40:38.334495Z",
     "iopub.status.busy": "2025-11-24T00:40:38.334072Z",
     "iopub.status.idle": "2025-11-24T00:40:45.209201Z",
     "shell.execute_reply": "2025-11-24T00:40:45.208495Z",
     "shell.execute_reply.started": "2025-11-24T00:40:38.334434Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== all_order_products.csv ===\n",
      "   order_id  product_id  add_to_cart_order\n",
      "0         2       33120                  1\n",
      "1         2       28985                  2\n",
      "2         2        9327                  3\n",
      "3         2       45918                  4\n",
      "4         2       30035                  5\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== orders.csv ===\n",
      "   order_id  user_id            timestamp\n",
      "0   2539329        1  2018-01-03 08:00:00\n",
      "1   2398795        1  2018-01-18 07:00:00\n",
      "2    473747        1  2018-02-08 12:00:00\n",
      "3   2254736        1  2018-03-09 07:00:00\n",
      "4    431534        1  2018-04-06 15:00:00\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== products.csv ===\n",
      "   product_id                                       product_name  aisle_id  \\\n",
      "0           1                         Chocolate Sandwich Cookies        61   \n",
      "1           2                                   All-Seasons Salt       104   \n",
      "2           3               Robust Golden Unsweetened Oolong Tea        94   \n",
      "3           4  Smart Ones Classic Favorites Mini Rigatoni Wit...        38   \n",
      "4           5                          Green Chile Anytime Sauce         5   \n",
      "\n",
      "   department_id  \n",
      "0             19  \n",
      "1             13  \n",
      "2              7  \n",
      "3              1  \n",
      "4             13  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the datasets\n",
    "order_products = pd.read_csv('/kaggle/input/simplifiedinstacartdata/all_order_products.csv')\n",
    "orders = pd.read_csv('/kaggle/input/simplifiedinstacartdata/orders.csv')\n",
    "products = pd.read_csv('/kaggle/input/simplifiedinstacartdata/products.csv')\n",
    "\n",
    "# Display the heads of each dataset\n",
    "print(\"=== all_order_products.csv ===\")\n",
    "print(order_products.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"=== orders.csv ===\")\n",
    "print(orders.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"=== products.csv ===\")\n",
    "print(products.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T00:40:51.965232Z",
     "iopub.status.busy": "2025-11-24T00:40:51.964932Z",
     "iopub.status.idle": "2025-11-24T00:42:07.283846Z",
     "shell.execute_reply": "2025-11-24T00:42:07.283150Z",
     "shell.execute_reply.started": "2025-11-24T00:40:51.965211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame:\n",
      "   user_id  order_no                             product_name\n",
      "0        1         1                                     Soda\n",
      "1        1         1  Organic Unsweetened Vanilla Almond Milk\n",
      "2        1         1                      Original Beef Jerky\n",
      "3        1         1               Aged White Cheddar Popcorn\n",
      "4        1         1         XL Pick-A-Size Paper Towel Rolls\n",
      "5        1         2                                     Soda\n",
      "6        1         2                               Pistachios\n",
      "7        1         2                      Original Beef Jerky\n",
      "8        1         2                   Bag of Organic Bananas\n",
      "9        1         2               Aged White Cheddar Popcorn\n",
      "\n",
      "DataFrame shape: (33819106, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "order_products = pd.read_csv('/kaggle/input/simplifiedinstacartdata/all_order_products.csv')\n",
    "orders = pd.read_csv('/kaggle/input/simplifiedinstacartdata/orders.csv')\n",
    "products = pd.read_csv('/kaggle/input/simplifiedinstacartdata/products.csv')\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "orders['timestamp'] = pd.to_datetime(orders['timestamp'])\n",
    "\n",
    "# Sort orders by user_id and timestamp to prepare for order numbering\n",
    "orders_sorted = orders.sort_values(['user_id', 'timestamp'])\n",
    "\n",
    "# Assign order sequence number for each user\n",
    "orders_sorted['order_no'] = orders_sorted.groupby('user_id').cumcount() + 1\n",
    "\n",
    "# Merge order_products with orders to get user_id and order_no\n",
    "merged_df = order_products.merge(\n",
    "    orders_sorted[['order_id', 'user_id', 'order_no']], \n",
    "    on='order_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with products to get product_name\n",
    "final_df = merged_df.merge(\n",
    "    products[['product_id', 'product_name']], \n",
    "    on='product_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select and reorder the required columns\n",
    "result_df = final_df[['user_id', 'order_no', 'product_name']]\n",
    "\n",
    "# Sort by user_id and order_no for better readability\n",
    "result_df = result_df.sort_values(['user_id', 'order_no']).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "result_df.to_csv('user_orders_products.csv', index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Final DataFrame:\")\n",
    "print(result_df.head(10))\n",
    "print(f\"\\nDataFrame shape: {result_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T00:42:39.020043Z",
     "iopub.status.busy": "2025-11-24T00:42:39.019484Z",
     "iopub.status.idle": "2025-11-24T00:42:39.033310Z",
     "shell.execute_reply": "2025-11-24T00:42:39.032605Z",
     "shell.execute_reply.started": "2025-11-24T00:42:39.020019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "food_question_dict = {\n",
    "    'Q6': 'tomatoes',\n",
    "    'Q9': 'lettuce or leafy greens',\n",
    "    'Q13': 'cabbage',\n",
    "    'Q14': 'sprouts',\n",
    "    'Q15_A': 'alfalfa sprouts',\n",
    "    'Q15_B': 'bean sprouts',\n",
    "    'Q16': 'cucumbers',\n",
    "    'Q17': 'bell peppers',\n",
    "    'Q18': 'hot peppers',\n",
    "    'Q19': 'celery',\n",
    "    'Q20': 'carrots',\n",
    "    'Q21': 'mini/baby carrots',\n",
    "    'Q22': 'peas',\n",
    "    'Q23': 'green or yellow beans',\n",
    "    'Q24': 'broccoli',\n",
    "    'Q25': 'cauliflower',\n",
    "    'Q26': 'leeks',\n",
    "    'Q27': 'fresh garlic',\n",
    "    'Q28': 'mushrooms',\n",
    "    'Q29': 'zucchini',\n",
    "    'Q30': 'onions',\n",
    "    'Q31_A': 'white onions/yellow onions',\n",
    "    'Q31_B': 'red onions',\n",
    "    'Q31_C': 'green onions',\n",
    "    'Q32': 'vegetable juices',\n",
    "    'Q33_A': 'fresh thai basil',\n",
    "    'Q33_B': 'fresh basil',\n",
    "    'Q33_C': 'fresh cilantro/coriander',\n",
    "    'Q33_D': 'fresh tarragon',\n",
    "    'Q33_E': 'fresh parsley',\n",
    "    'Q33_F': 'other fresh herbs',\n",
    "    'Q34_A': 'pepper',\n",
    "    'Q34_B': 'curry powder',\n",
    "    'Q34_C': 'paprika',\n",
    "    'Q34_D': 'turmeric',\n",
    "    'Q34_E': 'other spices',\n",
    "    'Q35': 'store-bought prepared salads',\n",
    "    'Q36_A': 'green salad',\n",
    "    'Q36_B': 'coleslaw',\n",
    "    'Q36_C': 'potato salad',\n",
    "    'Q36_D': 'pasta salad',\n",
    "    'Q36_E': 'fruit salad/pre-cut fruit or fruit platter',\n",
    "    'Q37A': 'store bought salsa',\n",
    "    'Q37_B': 'store bought hummus',\n",
    "    'Q38': 'apples',\n",
    "    'Q39': 'pears',\n",
    "    'Q40': 'peaches',\n",
    "    'Q41': 'nectarines',\n",
    "    'Q42': 'apricots',\n",
    "    'Q43': 'plums',\n",
    "    'Q44': 'citrus fruit',\n",
    "    'Q45': 'cherries',\n",
    "    'Q46': 'grapes',\n",
    "    'Q47': 'bananas',\n",
    "    'Q48': 'mangos',\n",
    "    'Q49': 'papayas',\n",
    "    'Q50': 'kiwi',\n",
    "    'Q51': 'pomegranate',\n",
    "    'Q52': 'pineapple',\n",
    "    'Q53': 'avocado',\n",
    "    'Q54': 'olives',\n",
    "    'Q55': 'cantaloupe',\n",
    "    'Q56': 'honeydew melon',\n",
    "    'Q57': 'watermelon',\n",
    "    'Q58': 'strawberries',\n",
    "    'Q59': 'raspberries',\n",
    "    'Q60': 'blueberries',\n",
    "    'Q61': 'blackberries',\n",
    "    'Q62': 'unpasteurized fruit juice',\n",
    "    'Q63': 'fruit smoothies',\n",
    "    'QN1_A': 'berries from the land',\n",
    "    'QN1_B': 'other plants',\n",
    "    'Q64': 'peanut butter',\n",
    "    'Q65': 'other nut paste, butter or spread',\n",
    "    'Q66': 'nuts',\n",
    "    'Q67_A': 'peanuts',\n",
    "    'Q67_B': 'almonds',\n",
    "    'Q67_C': 'walnuts',\n",
    "    'Q67_D': 'hazelnuts',\n",
    "    'Q67_E': 'cashews',\n",
    "    'Q67_F': 'pecans',\n",
    "    'Q68_A': 'sunflower seeds',\n",
    "    'Q68_B': 'sesame seeds',\n",
    "    'Q68_C': 'tahini, halva or other sesame products',\n",
    "    'Q107_GG': 'chia product or chia containing products',\n",
    "    'Q107_H': 'chia seeds',\n",
    "    'Q107_I': 'chia seed powder',\n",
    "    'Q69': 'beef',\n",
    "    'Q70': 'hamburgers',\n",
    "    'Q71': 'home-made hamburgers from ground beef',\n",
    "    'Q72': 'store-bought frozen beef patties',\n",
    "    'Q73': 'beef hamburgers from restaurant or fast food',\n",
    "    'Q74': 'other ground beef',\n",
    "    'Q75': 'raw or undercooked ground beef',\n",
    "    'Q76': 'raw beef',\n",
    "    'Q77': 'steak',\n",
    "    'Q78': 'stewing beef',\n",
    "    'Q79': 'other whole-cut beef products',\n",
    "    'Q80': 'pork',\n",
    "    'Q81_A': 'ham',\n",
    "    'Q81_B': 'bacon',\n",
    "    'Q81_C': 'ground pork',\n",
    "    'Q81_D': 'pork pieces or parts',\n",
    "    'Q82': 'chicken',\n",
    "    'Q83_A': 'store-bought breaded chicken',\n",
    "    'Q83_B': 'ground chicken',\n",
    "    'Q83_C': 'chicken pieces or parts',\n",
    "    'Q83_D': 'chicken from restaurant or fast food',\n",
    "    'Q84': 'turkey',\n",
    "    'Q85_A': 'turkey bacon',\n",
    "    'Q85_B': 'ground turkey',\n",
    "    'Q85_C': 'turkey pieces or parts',\n",
    "    'Q86': 'other poultry',\n",
    "    'Q87': 'deli-meat/cold cuts',\n",
    "    'Q88_A': 'chicken deli meat',\n",
    "    'Q88_B': 'turkey deli meat',\n",
    "    'Q88_C': 'ham deli meat',\n",
    "    'Q88_D': 'beef deli meat',\n",
    "    'Q88_E': 'bologna',\n",
    "    'Q88_F': 'salami',\n",
    "    'Q88_G': 'pepperoni',\n",
    "    'Q88_H': 'kielbasa',\n",
    "    'Q89_A': 'hot dogs',\n",
    "    'Q89_B': 'sausage',\n",
    "    'Q89_C': 'dried meat products',\n",
    "    'Q89_D': 'pâté/meat spread',\n",
    "    'Q89_E': 'lamb',\n",
    "    'Q89_F': 'veal',\n",
    "    'Q89_G': 'goat',\n",
    "    'Q89_H': 'organ meats or offal',\n",
    "    'Q90': 'shawarma or donair',\n",
    "    'Q91_1': 'chicken shawarma/donair',\n",
    "    'Q91_2': 'beef shawarma/donair',\n",
    "    'Q91_3': 'pork shawarma/donair',\n",
    "    'Q91_4': 'lamb shawarma/donair',\n",
    "    'Q91_5': 'turkey shawarma/donair',\n",
    "    'Q91_6': 'veal shawarma/donair',\n",
    "    'Q91_7': 'mixed meats shawarma/donair',\n",
    "    'Q91_8': 'other shawarma/donair',\n",
    "    'QN2_A': 'caribou',\n",
    "    'QN2_B': 'geese',\n",
    "    'QN2_C': 'duck',\n",
    "    'QN2_D': 'muskox',\n",
    "    'QN2_E': 'polar bear',\n",
    "    'QN2_F': 'seal',\n",
    "    'QN2_G': 'walrus',\n",
    "    'QN2_H': 'beluga',\n",
    "    'QN2_I': 'narwhal',\n",
    "    'QN2_J': 'bowhead',\n",
    "    'QN2_K': 'ptarmigan/grouse',\n",
    "    'QN2_L': 'moose',\n",
    "    'QN2_M': 'sheep',\n",
    "    'QN2_N': 'bear',\n",
    "    'QN2_O': 'bison',\n",
    "    'QN2_P': 'elk/deer',\n",
    "    'QN2_Q': 'gophers',\n",
    "    'QN2_R': 'beaver/muskrat',\n",
    "    'QN2_S': 'rabbit',\n",
    "    'Q93': 'fish',\n",
    "    'Q94_A': 'smoked fish',\n",
    "    'Q94_B': 'raw fish',\n",
    "    'Q95': 'shellfish',\n",
    "    'Q96_A': 'mussels',\n",
    "    'Q96_B': 'clams',\n",
    "    'Q96_C': 'scallops',\n",
    "    'Q96_D': 'shrimps/prawns',\n",
    "    'Q96_E': 'crab',\n",
    "    'Q96_F': 'lobster',\n",
    "    'Q97': 'oysters',\n",
    "    'Q98': 'raw oysters',\n",
    "    'QN3_A': 'arctic char',\n",
    "    'QN3_B': 'whitefish',\n",
    "    'QN3_C': 'trout',\n",
    "    'QN3_D': 'herring',\n",
    "    'QN3_E': 'inconnu',\n",
    "    'QN3_F': 'salmon',\n",
    "    'QN3_G': 'cod',\n",
    "    'QN3_H': 'seaweed',\n",
    "    'QN3_I': 'pike',\n",
    "    'Q99': 'eggs',\n",
    "    'Q100': 'raw or undercooked eggs',\n",
    "    'QN4_A': 'duck eggs',\n",
    "    'QN4_B': 'geese eggs',\n",
    "    'QN4_C': 'other wild eggs',\n",
    "    'Q101': 'dairy products',\n",
    "    'Q102_A': 'pasteurized dairy milk',\n",
    "    'Q102_B': 'unpasteurized dairy milk',\n",
    "    'Q102_C': 'powdered milk product',\n",
    "    'Q102_D': 'whipped/whipping cream',\n",
    "    'Q102_E': 'sour cream',\n",
    "    'Q102_F': 'ice cream/gelato',\n",
    "    'Q102_G': 'yogurt',\n",
    "    'Q103': 'dairy substitutes or non-dairy milk',\n",
    "    'Q104': 'cheese products',\n",
    "    'Q105_A': 'cheddar cheese',\n",
    "    'Q105_B': 'mozzarella',\n",
    "    'Q105_C': 'parmesan cheese',\n",
    "    'Q105_D': 'gouda',\n",
    "    'Q105_E': 'feta cheese',\n",
    "    'Q105_F': 'other block cheeses',\n",
    "    'Q105_G': 'brie, camembert or soft cheeses',\n",
    "    'Q105_H': 'blue-veined cheese',\n",
    "    'Q105_I': 'cottage, ricotta or fresh cheese',\n",
    "    'Q105_J': 'goat/sheep milk cheese',\n",
    "    'Q105_K': 'processed cheese',\n",
    "    'Q105_L': 'raw milk cheese',\n",
    "    'Q106_A': 'frozen vegetables',\n",
    "    'Q106_B': 'frozen berries',\n",
    "    'Q106_C': 'frozen fruit other than berries',\n",
    "    'Q106_D': 'frozen pizza',\n",
    "    'Q106_E': 'frozen pot pies',\n",
    "    'Q106_F': 'frozen meals',\n",
    "    'Q106_G': 'frozen snack foods/appetizers',\n",
    "    'Q107_A': 'dried fruit',\n",
    "    'Q107_B': 'granola bars, power bars, or protein bars',\n",
    "    'Q107_C': 'chips or pretzels',\n",
    "    'Q107_D': 'chocolate or chocolate-containing candy',\n",
    "    'Q107_E': 'cold breakfast cereal',\n",
    "    'Q107_F': 'hot breakfast cereal',\n",
    "    'Q107_G': 'tofu',\n",
    "    'Q108': 'dietary or nutritional supplement'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T00:43:11.006892Z",
     "iopub.status.busy": "2025-11-24T00:43:11.006598Z",
     "iopub.status.idle": "2025-11-24T00:43:11.014918Z",
     "shell.execute_reply": "2025-11-24T00:43:11.014188Z",
     "shell.execute_reply.started": "2025-11-24T00:43:11.006871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Question                Food_Item\n",
      "0       Q6                 tomatoes\n",
      "1       Q9  lettuce or leafy greens\n",
      "2      Q13                  cabbage\n",
      "3      Q14                  sprouts\n",
      "4    Q15_A          alfalfa sprouts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from the dictionary\n",
    "df = pd.DataFrame(list(food_question_dict.items()), columns=['Question', 'Food_Item'])\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Save to CSV if needed\n",
    "df.to_csv('food_question_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T01:22:22.896759Z",
     "iopub.status.busy": "2025-11-24T01:22:22.896112Z",
     "iopub.status.idle": "2025-11-24T01:25:56.454110Z",
     "shell.execute_reply": "2025-11-24T01:25:56.453252Z",
     "shell.execute_reply.started": "2025-11-24T01:22:22.896733Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets for hybrid categorization...\n",
      "Final dataframe loaded: 33,819,106 rows\n",
      "GPU Configuration: 2 GPU(s) detected\n",
      "HYBRID CATEGORIZATION PIPELINE\n",
      "======================================================================\n",
      "Initializing semantic model...\n",
      "Model initialized in 1.14 seconds\n",
      "Creating enhanced category descriptions...\n",
      "Created enhanced descriptions for 221 categories\n",
      "Computing category embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac243f97f35434d97766f9242d9e31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category embeddings computed in 0.19 seconds\n",
      "\n",
      "DATA ANALYSIS\n",
      "----------------------------------------\n",
      "Unique products to categorize: 49,685\n",
      "Total dataframe rows: 33,819,106\n",
      "Semantic threshold: 0.6\n",
      "Keyword threshold: 0.5\n",
      "\n",
      "HYBRID PROCESSING\n",
      "--------------------------------------------------\n",
      "Processing batch 1/50 (1,000 products)\n",
      "\n",
      "  HYBRID SAMPLE CATEGORIZATIONS (Batch 1):\n",
      "  --------------------------------------------------------------------------------\n",
      "    ✓ [semantic] Whole Vitamin D Milk                -> pasteurized dairy milk    (conf: 0.702)\n",
      "    ✓ [keyword ] Bag of Organic Bananas              -> bananas                   (conf: 0.700)\n",
      "    ✓ [keyword ] Organic Baby Spinach                -> spinach                   (conf: 0.900)\n",
      "Processing batch 2/50 (1,000 products)\n",
      "Processing batch 3/50 (1,000 products)\n",
      "Processing batch 10/50 (1,000 products)\n",
      "Processing batch 20/50 (1,000 products)\n",
      "Processing batch 30/50 (1,000 products)\n",
      "Processing batch 40/50 (1,000 products)\n",
      "Processing batch 50/50 (685 products)\n",
      "\n",
      "Hybrid processing completed in 51.57 seconds\n",
      "Method distribution: Semantic=10,526 Keyword=15,046 Ignore=24,113\n",
      "\n",
      "APPLYING HYBRID PREDICTIONS TO DATAFRAME\n",
      "--------------------------------------------------\n",
      "Dataframe integration completed in 16.04 seconds\n",
      "\n",
      "======================================================================\n",
      "HYBRID CATEGORIZATION COMPLETED\n",
      "======================================================================\n",
      "Total processing time: 74.71 seconds (1.25 minutes)\n",
      "Total rows processed: 33,819,106\n",
      "Products categorized: 24,432,499 (72.2%)\n",
      "Products ignored: 9,386,607 (27.8%)\n",
      "\n",
      "METHOD DISTRIBUTION:\n",
      "  Semantic: 10,526 (0.0%)\n",
      "  Keyword:  15,046 (0.0%)\n",
      "  Ignore:   24,113 (0.1%)\n",
      "\n",
      "CONFIDENCE STATISTICS:\n",
      "  Mean: 0.758\n",
      "  Std:  0.112\n",
      "\n",
      "TOP 20 CATEGORIES:\n",
      "------------------------------------------------------------\n",
      "  ignore                                        9,386,607 ( 27.8%)\n",
      "  citrus fruit                                  1,230,706 (  3.6%)\n",
      "  yogurt                                        1,126,808 (  3.3%)\n",
      "  bananas                                       1,009,571 (  3.0%)\n",
      "  apples                                         870,544 (  2.6%)\n",
      "  pasteurized dairy milk                         794,718 (  2.3%)\n",
      "  tomatoes                                       684,209 (  2.0%)\n",
      "  chips or pretzels                              645,477 (  1.9%)\n",
      "  avocado                                        595,755 (  1.8%)\n",
      "  chicken                                        545,081 (  1.6%)\n",
      "  chocolate or chocolate-containing candy        530,003 (  1.6%)\n",
      "  unpasteurized dairy milk                       525,297 (  1.6%)\n",
      "  strawberries                                   506,898 (  1.5%)\n",
      "  raspberries                                    485,485 (  1.4%)\n",
      "  cheese products                                473,931 (  1.4%)\n",
      "  bell peppers                                   465,336 (  1.4%)\n",
      "  lettuce or leafy greens                        433,744 (  1.3%)\n",
      "  almonds                                        430,553 (  1.3%)\n",
      "  green or yellow beans                          428,757 (  1.3%)\n",
      "  spinach                                        403,002 (  1.2%)\n",
      "\n",
      "CREATING MAPPING DATAFRAME...\n",
      "Created mapping dataframe with 49,685 unique products\n",
      "Sample of high-confidence mappings:\n",
      "  ✓ Apricots                                           -> apricots (conf: 1.000)\n",
      "  ✓ Fish                                               -> fish (conf: 1.000)\n",
      "  ✓ Gouda                                              -> gouda (conf: 1.000)\n",
      "  ✓ Peanut Butter                                      -> peanut butter (conf: 1.000)\n",
      "  ✓ Chicken                                            -> chicken (conf: 1.000)\n",
      "\n",
      "SAVING RESULTS...\n",
      "✓ Main categorized data saved: user_orders_products_categorized_hybrid.csv\n",
      "✓ Product-category mapping saved: product_category_mapping.csv\n",
      "✓ Sample mapping saved: product_category_mapping_sample.csv\n",
      "All files saved in 114.79 seconds\n",
      "\n",
      "======================================================================\n",
      "HYBRID CATEGORIZATION COMPLETED SUCCESSFULLY\n",
      "======================================================================\n",
      "Files created:\n",
      "  1. user_orders_products_categorized_hybrid.csv - Full dataset with categories\n",
      "  2. product_category_mapping.csv - Product to category mappings\n",
      "  3. product_category_mapping_sample.csv - Sample of mappings for review\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class HybridProductCategorizer:\n",
    "    \"\"\"\n",
    "    Hybrid categorization combining semantic similarity with keyword matching\n",
    "    for low-confidence classifications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, semantic_threshold=0.6, keyword_threshold=0.4):\n",
    "        self.semantic_threshold = semantic_threshold\n",
    "        self.keyword_threshold = keyword_threshold\n",
    "        self.device = self._setup_device()\n",
    "        self.model = None\n",
    "        self.food_embeddings = None\n",
    "        self.food_items = None\n",
    "        self.keyword_mappings = self._create_keyword_mappings()\n",
    "        \n",
    "    def _setup_device(self):\n",
    "        \"\"\"Initialize and configure computing device.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            print(f\"GPU Configuration: {gpu_count} GPU(s) detected\")\n",
    "            return torch.device('cuda')\n",
    "        else:\n",
    "            print(\"CPU Configuration: No GPU detected, using CPU\")\n",
    "            return torch.device('cpu')\n",
    "    \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Load and initialize the sentence transformer model.\"\"\"\n",
    "        print(\"Initializing semantic model...\")\n",
    "        start_time = time.time()\n",
    "        self.model = SentenceTransformer('all-mpnet-base-v2', device=self.device)\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"Model initialized in {load_time:.2f} seconds\")\n",
    "        return \"all-mpnet-base-v2\"\n",
    "    \n",
    "    def _create_keyword_mappings(self):\n",
    "        \"\"\"Create comprehensive keyword to category mappings.\"\"\"\n",
    "        return {\n",
    "            # Fruits\n",
    "            r'\\bbanana': 'bananas',\n",
    "            r'\\bapple': 'apples',\n",
    "            r'\\borange': 'citrus fruit',\n",
    "            r'\\blemon': 'citrus fruit',\n",
    "            r'\\blime': 'citrus fruit',\n",
    "            r'\\bgrapefruit': 'citrus fruit',\n",
    "            r'\\bgrape': 'grapes',\n",
    "            r'\\bstrawberr': 'strawberries',\n",
    "            r'\\bblueberr': 'blueberries',\n",
    "            r'\\braspberr': 'raspberries',\n",
    "            r'\\bblackberr': 'blackberries',\n",
    "            r'\\bcherr': 'cherries',\n",
    "            r'\\bpeach': 'peaches',\n",
    "            r'\\bpear': 'pears',\n",
    "            r'\\bplum': 'plums',\n",
    "            r'\\bavocado': 'avocado',\n",
    "            r'\\btomato': 'tomatoes',\n",
    "            r'\\bwatermelon': 'watermelon',\n",
    "            r'\\bcantaloupe': 'cantaloupe',\n",
    "            r'\\bpineapple': 'pineapple',\n",
    "            r'\\bkiwi': 'kiwi',\n",
    "            r'\\bmango': 'mangos',\n",
    "            \n",
    "            # Vegetables\n",
    "            r'\\bspinach': 'spinach',\n",
    "            r'\\blettuce': 'lettuce or leafy greens',\n",
    "            r'\\bkale': 'lettuce or leafy greens',\n",
    "            r'\\bcarrot': 'carrots',\n",
    "            r'\\bbroccoli': 'broccoli',\n",
    "            r'\\bcauliflower': 'cauliflower',\n",
    "            r'\\bcucumber': 'cucumbers',\n",
    "            r'\\bpepper': 'bell peppers',\n",
    "            r'\\bonion': 'onions',\n",
    "            r'\\bgarlic': 'fresh garlic',\n",
    "            r'\\bmushroom': 'mushrooms',\n",
    "            r'\\bcelery': 'celery',\n",
    "            r'\\bzucchini': 'zucchini',\n",
    "            r'\\bcabbage': 'cabbage',\n",
    "            r'\\bbean': 'green or yellow beans',\n",
    "            r'\\bpea': 'peas',\n",
    "            r'\\bsprout': 'sprouts',\n",
    "            \n",
    "            # Dairy & Alternatives\n",
    "            r'\\bmilk\\b': 'pasteurized dairy milk',\n",
    "            r'\\byogurt': 'yogurt',\n",
    "            r'\\bcheese': 'cheese products',\n",
    "            r'\\bcream': 'whipped/whipping cream',\n",
    "            r'\\balmond milk': 'dairy substitutes or non-dairy milk',\n",
    "            r'\\bsoy milk': 'dairy substitutes or non-dairy milk',\n",
    "            r'\\bcoconut milk': 'dairy substitutes or non-dairy milk',\n",
    "            r'\\boat milk': 'dairy substitutes or non-dairy milk',\n",
    "            \n",
    "            # Meat & Protein\n",
    "            r'\\bchicken': 'chicken',\n",
    "            r'\\bbeef': 'beef',\n",
    "            r'\\bpork': 'pork',\n",
    "            r'\\bturkey': 'turkey',\n",
    "            r'\\bfish': 'fish',\n",
    "            r'\\bsalmon': 'fish',\n",
    "            r'\\btuna': 'fish',\n",
    "            r'\\bshrimp': 'shrimps/prawns',\n",
    "            r'\\bbacon': 'bacon',\n",
    "            r'\\bsausage': 'sausage',\n",
    "            r'\\bham': 'ham',\n",
    "            r'\\begg': 'eggs',\n",
    "            \n",
    "            # Other\n",
    "            r'\\bchips': 'chips or pretzels',\n",
    "            r'\\bpretzel': 'chips or pretzels',\n",
    "            r'\\bchocolate': 'chocolate or chocolate-containing candy',\n",
    "            r'\\bnut\\b': 'nuts',\n",
    "            r'\\bseed': 'sunflower seeds',\n",
    "            r'\\bpeanut': 'peanuts',\n",
    "            r'\\balmond': 'almonds',\n",
    "            r'\\bwalnut': 'walnuts',\n",
    "            r'\\bcashew': 'cashews',\n",
    "            r'\\bpecan': 'pecans',\n",
    "        }\n",
    "    \n",
    "    def create_enhanced_category_descriptions(self, food_question_df):\n",
    "        \"\"\"Create enhanced category descriptions for semantic matching.\"\"\"\n",
    "        print(\"Creating enhanced category descriptions...\")\n",
    "        \n",
    "        enhanced_descriptions = {\n",
    "            'pasteurized dairy milk': 'milk dairy beverage drink whole reduced fat 2% 1% skim vitamin d',\n",
    "            'dairy substitutes or non-dairy milk': 'almond milk soy milk coconut milk oat milk rice milk non-dairy plant-based alternative',\n",
    "            'yogurt': 'yogurt greek yogurt dairy cultured probiotic',\n",
    "            'cheese products': 'cheese dairy cheddar mozzarella swiss provolone',\n",
    "            'bananas': 'bananas fruit yellow ripe',\n",
    "            'spinach': 'spinach leafy greens vegetable green',\n",
    "            'tomatoes': 'tomatoes fruit vegetable red',\n",
    "            'apples': 'apples fruit red green',\n",
    "            'citrus fruit': 'oranges lemons limes grapefruit citrus',\n",
    "            'strawberries': 'strawberries berries fruit red',\n",
    "        }\n",
    "        \n",
    "        self.food_items = food_question_df['Food_Item'].unique().tolist()\n",
    "        enhanced_categories = []\n",
    "        \n",
    "        for category in self.food_items:\n",
    "            if category in enhanced_descriptions:\n",
    "                enhanced_categories.append(enhanced_descriptions[category])\n",
    "            else:\n",
    "                enhanced_categories.append(category)\n",
    "        \n",
    "        print(f\"Created enhanced descriptions for {len(enhanced_categories)} categories\")\n",
    "        return enhanced_categories\n",
    "    \n",
    "    def keyword_categorization(self, product_name):\n",
    "        \"\"\"Categorize product using keyword matching.\"\"\"\n",
    "        if pd.isna(product_name):\n",
    "            return None, 0.0\n",
    "        \n",
    "        product_lower = product_name.lower()\n",
    "        best_match = None\n",
    "        best_confidence = 0.0\n",
    "        \n",
    "        for pattern, category in self.keyword_mappings.items():\n",
    "            if re.search(pattern, product_lower):\n",
    "                # Calculate confidence based on match quality\n",
    "                confidence = 0.7  # Base confidence for keyword matches\n",
    "                \n",
    "                # Boost confidence for exact matches\n",
    "                if pattern.replace(r'\\b', '') in product_lower.split():\n",
    "                    confidence = 0.9\n",
    "                \n",
    "                if confidence > best_confidence:\n",
    "                    best_match = category\n",
    "                    best_confidence = confidence\n",
    "        \n",
    "        return best_match, best_confidence\n",
    "    \n",
    "    def semantic_categorization(self, product_names):\n",
    "        \"\"\"Categorize products using semantic similarity.\"\"\"\n",
    "        if isinstance(product_names, str):\n",
    "            product_names = [product_names]\n",
    "        \n",
    "        try:\n",
    "            product_embeddings = self.model.encode(\n",
    "                product_names, \n",
    "                convert_to_tensor=True, \n",
    "                device=self.device,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "            \n",
    "            similarities = util.cos_sim(product_embeddings, self.food_embeddings)\n",
    "            max_similarities, max_indices = torch.max(similarities, dim=1)\n",
    "            \n",
    "            categories = []\n",
    "            confidences = []\n",
    "            \n",
    "            for i, similarity in enumerate(max_similarities):\n",
    "                if similarity > self.semantic_threshold:\n",
    "                    categories.append(self.food_items[max_indices[i]])\n",
    "                    confidences.append(float(similarity))\n",
    "                else:\n",
    "                    categories.append(None)  # Mark for keyword fallback\n",
    "                    confidences.append(float(similarity))\n",
    "            \n",
    "            return categories, confidences\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic categorization: {e}\")\n",
    "            return [None] * len(product_names), [0.0] * len(product_names)\n",
    "    \n",
    "    def hybrid_categorization(self, product_names):\n",
    "        \"\"\"Hybrid categorization combining semantic and keyword approaches.\"\"\"\n",
    "        if isinstance(product_names, str):\n",
    "            product_names = [product_names]\n",
    "        \n",
    "        # First pass: semantic categorization\n",
    "        semantic_categories, semantic_confidences = self.semantic_categorization(product_names)\n",
    "        \n",
    "        final_categories = []\n",
    "        final_confidences = []\n",
    "        method_used = []  # Track which method was used\n",
    "        \n",
    "        for i, (product, semantic_cat, semantic_conf) in enumerate(zip(product_names, semantic_categories, semantic_confidences)):\n",
    "            \n",
    "            # If semantic confidence is high, use it\n",
    "            if semantic_conf > self.semantic_threshold:\n",
    "                final_categories.append(semantic_cat)\n",
    "                final_confidences.append(semantic_conf)\n",
    "                method_used.append(\"semantic\")\n",
    "                \n",
    "            else:\n",
    "                # Fall back to keyword matching\n",
    "                keyword_cat, keyword_conf = self.keyword_categorization(product)\n",
    "                \n",
    "                if keyword_cat and keyword_conf > self.keyword_threshold:\n",
    "                    final_categories.append(keyword_cat)\n",
    "                    final_confidences.append(keyword_conf)\n",
    "                    method_used.append(\"keyword\")\n",
    "                else:\n",
    "                    final_categories.append(\"ignore\")\n",
    "                    final_confidences.append(max(semantic_conf, keyword_conf))\n",
    "                    method_used.append(\"ignore\")\n",
    "        \n",
    "        return final_categories, final_confidences, method_used\n",
    "    \n",
    "    def categorize_all_products_hybrid(self, final_df, food_question_df):\n",
    "        \"\"\"\n",
    "        Categorize all products using hybrid approach.\n",
    "        \"\"\"\n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        print(\"HYBRID CATEGORIZATION PIPELINE\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Initialize model\n",
    "        model_name = self.initialize_model()\n",
    "        enhanced_categories = self.create_enhanced_category_descriptions(food_question_df)\n",
    "        \n",
    "        # Precompute category embeddings\n",
    "        print(\"Computing category embeddings...\")\n",
    "        start_time = time.time()\n",
    "        self.food_embeddings = self.model.encode(\n",
    "            enhanced_categories, \n",
    "            convert_to_tensor=True, \n",
    "            device=self.device,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        embed_time = time.time() - start_time\n",
    "        print(f\"Category embeddings computed in {embed_time:.2f} seconds\")\n",
    "        \n",
    "        # Analyze input data\n",
    "        print(\"\\nDATA ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        unique_products = final_df['product_name'].dropna().unique()\n",
    "        total_unique_products = len(unique_products)\n",
    "        total_dataframe_rows = len(final_df)\n",
    "        \n",
    "        print(f\"Unique products to categorize: {total_unique_products:,}\")\n",
    "        print(f\"Total dataframe rows: {total_dataframe_rows:,}\")\n",
    "        print(f\"Semantic threshold: {self.semantic_threshold}\")\n",
    "        print(f\"Keyword threshold: {self.keyword_threshold}\")\n",
    "        \n",
    "        # Process all products with hybrid approach\n",
    "        print(\"\\nHYBRID PROCESSING\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        processing_start_time = time.time()\n",
    "        product_to_category = {}\n",
    "        product_confidence = {}\n",
    "        product_method = {}\n",
    "        \n",
    "        batch_size = 1000\n",
    "        total_batches = (total_unique_products + batch_size - 1) // batch_size\n",
    "        \n",
    "        semantic_count = 0\n",
    "        keyword_count = 0\n",
    "        ignore_count = 0\n",
    "        \n",
    "        for batch_idx in range(0, total_unique_products, batch_size):\n",
    "            batch_num = (batch_idx // batch_size) + 1\n",
    "            batch_products = unique_products[batch_idx:batch_idx + batch_size]\n",
    "            \n",
    "            if batch_num % 10 == 0 or batch_num <= 3:\n",
    "                print(f\"Processing batch {batch_num}/{total_batches} ({len(batch_products):,} products)\")\n",
    "            \n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Categorize batch with hybrid approach\n",
    "            batch_categories, batch_confidences, batch_methods = self.hybrid_categorization(batch_products)\n",
    "            \n",
    "            # Update mappings and count methods\n",
    "            for product, category, confidence, method in zip(batch_products, batch_categories, batch_confidences, batch_methods):\n",
    "                product_to_category[product] = category\n",
    "                product_confidence[product] = confidence\n",
    "                product_method[product] = method\n",
    "                \n",
    "                if method == \"semantic\":\n",
    "                    semantic_count += 1\n",
    "                elif method == \"keyword\":\n",
    "                    keyword_count += 1\n",
    "                else:\n",
    "                    ignore_count += 1\n",
    "            \n",
    "            batch_time = time.time() - batch_start_time\n",
    "            \n",
    "            # Show samples from first batch\n",
    "            if batch_num == 1:\n",
    "                print(f\"\\n  HYBRID SAMPLE CATEGORIZATIONS (Batch 1):\")\n",
    "                print(\"  \" + \"-\" * 80)\n",
    "                sample_products = [\n",
    "                    \"Whole Vitamin D Milk\",\n",
    "                    \"Bag of Organic Bananas\", \n",
    "                    \"Organic Baby Spinach\",\n",
    "                    \"Almond Milk\",\n",
    "                    \"Fresh Spinach\",\n",
    "                    \"Greek Yogurt\"\n",
    "                ]\n",
    "                for test_product in sample_products:\n",
    "                    if test_product in product_to_category:\n",
    "                        category = product_to_category[test_product]\n",
    "                        confidence = product_confidence[test_product]\n",
    "                        method = product_method[test_product]\n",
    "                        status = \"✓\" if category != \"ignore\" else \"✗\"\n",
    "                        print(f\"    {status} [{method:<8}] {test_product:<35} -> {category:<25} (conf: {confidence:.3f})\")\n",
    "        \n",
    "        processing_time = time.time() - processing_start_time\n",
    "        print(f\"\\nHybrid processing completed in {processing_time:.2f} seconds\")\n",
    "        print(f\"Method distribution: Semantic={semantic_count:,} Keyword={keyword_count:,} Ignore={ignore_count:,}\")\n",
    "        \n",
    "        # Apply to dataframe\n",
    "        print(\"\\nAPPLYING HYBRID PREDICTIONS TO DATAFRAME\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        integration_start_time = time.time()\n",
    "        categorized_df = final_df.copy()\n",
    "        categorized_df['food_category'] = categorized_df['product_name'].map(product_to_category)\n",
    "        categorized_df['food_category'] = categorized_df['food_category'].fillna('ignore')\n",
    "        categorized_df['confidence'] = categorized_df['product_name'].map(product_confidence)\n",
    "        categorized_df['confidence'] = categorized_df['confidence'].fillna(0.0)\n",
    "        categorized_df['method'] = categorized_df['product_name'].map(product_method)\n",
    "        categorized_df['method'] = categorized_df['method'].fillna('ignore')\n",
    "        \n",
    "        integration_time = time.time() - integration_start_time\n",
    "        print(f\"Dataframe integration completed in {integration_time:.2f} seconds\")\n",
    "        \n",
    "        # Generate statistics\n",
    "        self._generate_hybrid_statistics(categorized_df, total_dataframe_rows, total_start_time, semantic_count, keyword_count, ignore_count)\n",
    "        \n",
    "        # Create downloadable mapping\n",
    "        mapping_df = self._create_mapping_dataframe(product_to_category, product_confidence, product_method)\n",
    "        \n",
    "        return categorized_df, mapping_df\n",
    "    \n",
    "    def _generate_hybrid_statistics(self, categorized_df, total_rows, start_time, semantic_count, keyword_count, ignore_count):\n",
    "        \"\"\"Generate comprehensive statistics.\"\"\"\n",
    "        total_time = time.time() - start_time\n",
    "        category_counts = categorized_df['food_category'].value_counts()\n",
    "        ignore_count_total = category_counts.get('ignore', 0)\n",
    "        categorized_count = total_rows - ignore_count_total\n",
    "        \n",
    "        method_counts = categorized_df['method'].value_counts()\n",
    "        confidence_stats = categorized_df[categorized_df['food_category'] != 'ignore']['confidence'].describe()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"HYBRID CATEGORIZATION COMPLETED\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Total processing time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "        print(f\"Total rows processed: {total_rows:,}\")\n",
    "        print(f\"Products categorized: {categorized_count:,} ({categorized_count/total_rows*100:.1f}%)\")\n",
    "        print(f\"Products ignored: {ignore_count_total:,} ({ignore_count_total/total_rows*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nMETHOD DISTRIBUTION:\")\n",
    "        print(f\"  Semantic: {semantic_count:,} ({semantic_count/total_rows*100:.1f}%)\")\n",
    "        print(f\"  Keyword:  {keyword_count:,} ({keyword_count/total_rows*100:.1f}%)\")\n",
    "        print(f\"  Ignore:   {ignore_count:,} ({ignore_count/total_rows*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nCONFIDENCE STATISTICS:\")\n",
    "        print(f\"  Mean: {confidence_stats['mean']:.3f}\")\n",
    "        print(f\"  Std:  {confidence_stats['std']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nTOP 20 CATEGORIES:\")\n",
    "        print(\"-\" * 60)\n",
    "        for category, count in category_counts.head(20).items():\n",
    "            percentage = count / total_rows * 100\n",
    "            print(f\"  {category:<45} {count:>8,} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    def _create_mapping_dataframe(self, product_to_category, product_confidence, product_method):\n",
    "        \"\"\"Create a downloadable dataframe with product to category mappings.\"\"\"\n",
    "        print(\"\\nCREATING MAPPING DATAFRAME...\")\n",
    "        \n",
    "        mapping_data = []\n",
    "        for product, category in product_to_category.items():\n",
    "            mapping_data.append({\n",
    "                'product_name': product,\n",
    "                'food_category': category,\n",
    "                'confidence': product_confidence.get(product, 0.0),\n",
    "                'method': product_method.get(product, 'unknown')\n",
    "            })\n",
    "        \n",
    "        mapping_df = pd.DataFrame(mapping_data)\n",
    "        \n",
    "        # Sort by confidence for easier analysis\n",
    "        mapping_df = mapping_df.sort_values('confidence', ascending=False)\n",
    "        \n",
    "        print(f\"Created mapping dataframe with {len(mapping_df):,} unique products\")\n",
    "        print(f\"Sample of high-confidence mappings:\")\n",
    "        high_conf_samples = mapping_df[mapping_df['confidence'] > 0.8].head(5)\n",
    "        for _, row in high_conf_samples.iterrows():\n",
    "            print(f\"  ✓ {row['product_name'][:50]:<50} -> {row['food_category']} (conf: {row['confidence']:.3f})\")\n",
    "        \n",
    "        return mapping_df\n",
    "\n",
    "\n",
    "def main_hybrid_categorization():\n",
    "    \"\"\"Main execution with hybrid categorization.\"\"\"\n",
    "    try:\n",
    "        print(\"Loading datasets for hybrid categorization...\")\n",
    "        \n",
    "        final_df = pd.read_csv('user_orders_products.csv')\n",
    "        food_question_df = pd.DataFrame(list(food_question_dict.items()), \n",
    "                                      columns=['Question', 'Food_Item'])\n",
    "        \n",
    "        print(f\"Final dataframe loaded: {len(final_df):,} rows\")\n",
    "        \n",
    "        # Initialize hybrid categorizer with optimized thresholds\n",
    "        categorizer = HybridProductCategorizer(\n",
    "            semantic_threshold=0.6,\n",
    "            keyword_threshold=0.5\n",
    "        )\n",
    "        \n",
    "        # Execute hybrid categorization\n",
    "        categorized_results, mapping_df = categorizer.categorize_all_products_hybrid(\n",
    "            final_df, food_question_df\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        print(\"\\nSAVING RESULTS...\")\n",
    "        save_start = time.time()\n",
    "        \n",
    "        # Save the main categorized dataframe\n",
    "        categorized_results.to_csv('user_orders_products_categorized_hybrid.csv', index=False)\n",
    "        print(\"✓ Main categorized data saved: user_orders_products_categorized_hybrid.csv\")\n",
    "        \n",
    "        # Save the product-category mapping dataframe\n",
    "        mapping_df.to_csv('product_category_mapping.csv', index=False)\n",
    "        print(\"✓ Product-category mapping saved: product_category_mapping.csv\")\n",
    "        \n",
    "        # Save a sample for quick review\n",
    "        sample_mapping = mapping_df.head(1000)\n",
    "        sample_mapping.to_csv('product_category_mapping_sample.csv', index=False)\n",
    "        print(\"✓ Sample mapping saved: product_category_mapping_sample.csv\")\n",
    "        \n",
    "        save_time = time.time() - save_start\n",
    "        print(f\"All files saved in {save_time:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"HYBRID CATEGORIZATION COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Files created:\")\n",
    "        print(\"  1. user_orders_products_categorized_hybrid.csv - Full dataset with categories\")\n",
    "        print(\"  2. product_category_mapping.csv - Product to category mappings\")\n",
    "        print(\"  3. product_category_mapping_sample.csv - Sample of mappings for review\")\n",
    "        \n",
    "        return categorized_results, mapping_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in hybrid categorization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hybrid_results, mapping_df = main_hybrid_categorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T01:26:20.350224Z",
     "iopub.status.busy": "2025-11-24T01:26:20.349957Z",
     "iopub.status.idle": "2025-11-24T01:26:20.807606Z",
     "shell.execute_reply": "2025-11-24T01:26:20.806540Z",
     "shell.execute_reply.started": "2025-11-24T01:26:20.350205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/test1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T01:34:33.768491Z",
     "iopub.status.busy": "2025-11-24T01:34:33.767782Z",
     "iopub.status.idle": "2025-11-24T01:35:40.597327Z",
     "shell.execute_reply": "2025-11-24T01:35:40.596667Z",
     "shell.execute_reply.started": "2025-11-24T01:34:33.768421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Original dataset: 33,819,106 rows\n",
      "Mapping dataset: 49,685 unique products\n",
      "\n",
      "Merging datasets...\n",
      "\n",
      "First 10 rows of the final dataset:\n",
      "   user_id  order_no                        food_category\n",
      "0        1         1                               ignore\n",
      "1        1         1  dairy substitutes or non-dairy milk\n",
      "2        1         1                       beef deli meat\n",
      "3        1         1                               ignore\n",
      "4        1         1                               ignore\n",
      "5        1         2                               ignore\n",
      "6        1         2                              almonds\n",
      "7        1         2                       beef deli meat\n",
      "8        1         2                              bananas\n",
      "9        1         2                               ignore\n",
      "\n",
      "Saving final file...\n",
      "✓ File saved: user_orders_product_food_category.csv\n",
      "\n",
      "Dataset statistics:\n",
      "Total rows: 33,819,106\n",
      "Unique users: 206,209\n",
      "Unique food categories: 196\n",
      "Most common categories:\n",
      "food_category\n",
      "ignore                    9386607\n",
      "citrus fruit              1230706\n",
      "yogurt                    1126808\n",
      "bananas                   1009571\n",
      "apples                     870544\n",
      "pasteurized dairy milk     794718\n",
      "tomatoes                   684209\n",
      "chips or pretzels          645477\n",
      "avocado                    595755\n",
      "chicken                    545081\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "original_df = pd.read_csv('user_orders_products.csv')\n",
    "mapping_df = pd.read_csv('product_category_mapping.csv')\n",
    "\n",
    "print(f\"Original dataset: {len(original_df):,} rows\")\n",
    "print(f\"Mapping dataset: {len(mapping_df):,} unique products\")\n",
    "\n",
    "# Merge to add food_category\n",
    "print(\"\\nMerging datasets...\")\n",
    "final_df = original_df.merge(\n",
    "    mapping_df[['product_name', 'food_category']],\n",
    "    on='product_name',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill any missing categories with 'ignore'\n",
    "final_df['food_category'] = final_df['food_category'].fillna('ignore')\n",
    "\n",
    "# Select only the required columns\n",
    "final_df = final_df[['user_id', 'order_no', 'food_category']]\n",
    "\n",
    "# Display the head\n",
    "print(\"\\nFirst 10 rows of the final dataset:\")\n",
    "print(final_df.head(10))\n",
    "\n",
    "# Save the final file\n",
    "print(\"\\nSaving final file...\")\n",
    "final_df.to_csv('user_orders_product_food_category.csv', index=False)\n",
    "print(\"✓ File saved: user_orders_product_food_category.csv\")\n",
    "\n",
    "# Show some statistics\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"Total rows: {len(final_df):,}\")\n",
    "print(f\"Unique users: {final_df['user_id'].nunique():,}\")\n",
    "print(f\"Unique food categories: {final_df['food_category'].nunique()}\")\n",
    "print(f\"Most common categories:\")\n",
    "print(final_df['food_category'].value_counts().head(10))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2039650,
     "sourceId": 3401731,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
