{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3401731,"sourceType":"datasetVersion","datasetId":2039650}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Read the datasets\norder_products = pd.read_csv('/kaggle/input/simplifiedinstacartdata/all_order_products.csv')\norders = pd.read_csv('/kaggle/input/simplifiedinstacartdata/orders.csv')\nproducts = pd.read_csv('/kaggle/input/simplifiedinstacartdata/products.csv')\n\n# Display the heads of each dataset\nprint(\"=== all_order_products.csv ===\")\nprint(order_products.head())\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\nprint(\"=== orders.csv ===\")\nprint(orders.head())\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\nprint(\"=== products.csv ===\")\nprint(products.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T00:40:38.334072Z","iopub.execute_input":"2025-11-24T00:40:38.334495Z","iopub.status.idle":"2025-11-24T00:40:45.209201Z","shell.execute_reply.started":"2025-11-24T00:40:38.334434Z","shell.execute_reply":"2025-11-24T00:40:45.208495Z"}},"outputs":[{"name":"stdout","text":"=== all_order_products.csv ===\n   order_id  product_id  add_to_cart_order\n0         2       33120                  1\n1         2       28985                  2\n2         2        9327                  3\n3         2       45918                  4\n4         2       30035                  5\n\n==================================================\n\n=== orders.csv ===\n   order_id  user_id            timestamp\n0   2539329        1  2018-01-03 08:00:00\n1   2398795        1  2018-01-18 07:00:00\n2    473747        1  2018-02-08 12:00:00\n3   2254736        1  2018-03-09 07:00:00\n4    431534        1  2018-04-06 15:00:00\n\n==================================================\n\n=== products.csv ===\n   product_id                                       product_name  aisle_id  \\\n0           1                         Chocolate Sandwich Cookies        61   \n1           2                                   All-Seasons Salt       104   \n2           3               Robust Golden Unsweetened Oolong Tea        94   \n3           4  Smart Ones Classic Favorites Mini Rigatoni Wit...        38   \n4           5                          Green Chile Anytime Sauce         5   \n\n   department_id  \n0             19  \n1             13  \n2              7  \n3              1  \n4             13  \n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the datasets\norder_products = pd.read_csv('/kaggle/input/simplifiedinstacartdata/all_order_products.csv')\norders = pd.read_csv('/kaggle/input/simplifiedinstacartdata/orders.csv')\nproducts = pd.read_csv('/kaggle/input/simplifiedinstacartdata/products.csv')\n\n# Convert timestamp to datetime\norders['timestamp'] = pd.to_datetime(orders['timestamp'])\n\n# Sort orders by user_id and timestamp to prepare for order numbering\norders_sorted = orders.sort_values(['user_id', 'timestamp'])\n\n# Assign order sequence number for each user\norders_sorted['order_no'] = orders_sorted.groupby('user_id').cumcount() + 1\n\n# Merge order_products with orders to get user_id and order_no\nmerged_df = order_products.merge(\n    orders_sorted[['order_id', 'user_id', 'order_no']], \n    on='order_id', \n    how='left'\n)\n\n# Merge with products to get product_name\nfinal_df = merged_df.merge(\n    products[['product_id', 'product_name']], \n    on='product_id', \n    how='left'\n)\n\n# Select and reorder the required columns\nresult_df = final_df[['user_id', 'order_no', 'product_name']]\n\n# Sort by user_id and order_no for better readability\nresult_df = result_df.sort_values(['user_id', 'order_no']).reset_index(drop=True)\n\n# Save to CSV\nresult_df.to_csv('user_orders_products.csv', index=False)\n\n# Display the first few rows\nprint(\"Final DataFrame:\")\nprint(result_df.head(10))\nprint(f\"\\nDataFrame shape: {result_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T00:40:51.964932Z","iopub.execute_input":"2025-11-24T00:40:51.965232Z","iopub.status.idle":"2025-11-24T00:42:07.283846Z","shell.execute_reply.started":"2025-11-24T00:40:51.965211Z","shell.execute_reply":"2025-11-24T00:42:07.283150Z"}},"outputs":[{"name":"stdout","text":"Final DataFrame:\n   user_id  order_no                             product_name\n0        1         1                                     Soda\n1        1         1  Organic Unsweetened Vanilla Almond Milk\n2        1         1                      Original Beef Jerky\n3        1         1               Aged White Cheddar Popcorn\n4        1         1         XL Pick-A-Size Paper Towel Rolls\n5        1         2                                     Soda\n6        1         2                               Pistachios\n7        1         2                      Original Beef Jerky\n8        1         2                   Bag of Organic Bananas\n9        1         2               Aged White Cheddar Popcorn\n\nDataFrame shape: (33819106, 3)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"food_question_dict = {\n    'Q6': 'tomatoes',\n    'Q9': 'lettuce or leafy greens',\n    'Q13': 'cabbage',\n    'Q14': 'sprouts',\n    'Q15_A': 'alfalfa sprouts',\n    'Q15_B': 'bean sprouts',\n    'Q16': 'cucumbers',\n    'Q17': 'bell peppers',\n    'Q18': 'hot peppers',\n    'Q19': 'celery',\n    'Q20': 'carrots',\n    'Q21': 'mini/baby carrots',\n    'Q22': 'peas',\n    'Q23': 'green or yellow beans',\n    'Q24': 'broccoli',\n    'Q25': 'cauliflower',\n    'Q26': 'leeks',\n    'Q27': 'fresh garlic',\n    'Q28': 'mushrooms',\n    'Q29': 'zucchini',\n    'Q30': 'onions',\n    'Q31_A': 'white onions/yellow onions',\n    'Q31_B': 'red onions',\n    'Q31_C': 'green onions',\n    'Q32': 'vegetable juices',\n    'Q33_A': 'fresh thai basil',\n    'Q33_B': 'fresh basil',\n    'Q33_C': 'fresh cilantro/coriander',\n    'Q33_D': 'fresh tarragon',\n    'Q33_E': 'fresh parsley',\n    'Q33_F': 'other fresh herbs',\n    'Q34_A': 'pepper',\n    'Q34_B': 'curry powder',\n    'Q34_C': 'paprika',\n    'Q34_D': 'turmeric',\n    'Q34_E': 'other spices',\n    'Q35': 'store-bought prepared salads',\n    'Q36_A': 'green salad',\n    'Q36_B': 'coleslaw',\n    'Q36_C': 'potato salad',\n    'Q36_D': 'pasta salad',\n    'Q36_E': 'fruit salad/pre-cut fruit or fruit platter',\n    'Q37A': 'store bought salsa',\n    'Q37_B': 'store bought hummus',\n    'Q38': 'apples',\n    'Q39': 'pears',\n    'Q40': 'peaches',\n    'Q41': 'nectarines',\n    'Q42': 'apricots',\n    'Q43': 'plums',\n    'Q44': 'citrus fruit',\n    'Q45': 'cherries',\n    'Q46': 'grapes',\n    'Q47': 'bananas',\n    'Q48': 'mangos',\n    'Q49': 'papayas',\n    'Q50': 'kiwi',\n    'Q51': 'pomegranate',\n    'Q52': 'pineapple',\n    'Q53': 'avocado',\n    'Q54': 'olives',\n    'Q55': 'cantaloupe',\n    'Q56': 'honeydew melon',\n    'Q57': 'watermelon',\n    'Q58': 'strawberries',\n    'Q59': 'raspberries',\n    'Q60': 'blueberries',\n    'Q61': 'blackberries',\n    'Q62': 'unpasteurized fruit juice',\n    'Q63': 'fruit smoothies',\n    'QN1_A': 'berries from the land',\n    'QN1_B': 'other plants',\n    'Q64': 'peanut butter',\n    'Q65': 'other nut paste, butter or spread',\n    'Q66': 'nuts',\n    'Q67_A': 'peanuts',\n    'Q67_B': 'almonds',\n    'Q67_C': 'walnuts',\n    'Q67_D': 'hazelnuts',\n    'Q67_E': 'cashews',\n    'Q67_F': 'pecans',\n    'Q68_A': 'sunflower seeds',\n    'Q68_B': 'sesame seeds',\n    'Q68_C': 'tahini, halva or other sesame products',\n    'Q107_GG': 'chia product or chia containing products',\n    'Q107_H': 'chia seeds',\n    'Q107_I': 'chia seed powder',\n    'Q69': 'beef',\n    'Q70': 'hamburgers',\n    'Q71': 'home-made hamburgers from ground beef',\n    'Q72': 'store-bought frozen beef patties',\n    'Q73': 'beef hamburgers from restaurant or fast food',\n    'Q74': 'other ground beef',\n    'Q75': 'raw or undercooked ground beef',\n    'Q76': 'raw beef',\n    'Q77': 'steak',\n    'Q78': 'stewing beef',\n    'Q79': 'other whole-cut beef products',\n    'Q80': 'pork',\n    'Q81_A': 'ham',\n    'Q81_B': 'bacon',\n    'Q81_C': 'ground pork',\n    'Q81_D': 'pork pieces or parts',\n    'Q82': 'chicken',\n    'Q83_A': 'store-bought breaded chicken',\n    'Q83_B': 'ground chicken',\n    'Q83_C': 'chicken pieces or parts',\n    'Q83_D': 'chicken from restaurant or fast food',\n    'Q84': 'turkey',\n    'Q85_A': 'turkey bacon',\n    'Q85_B': 'ground turkey',\n    'Q85_C': 'turkey pieces or parts',\n    'Q86': 'other poultry',\n    'Q87': 'deli-meat/cold cuts',\n    'Q88_A': 'chicken deli meat',\n    'Q88_B': 'turkey deli meat',\n    'Q88_C': 'ham deli meat',\n    'Q88_D': 'beef deli meat',\n    'Q88_E': 'bologna',\n    'Q88_F': 'salami',\n    'Q88_G': 'pepperoni',\n    'Q88_H': 'kielbasa',\n    'Q89_A': 'hot dogs',\n    'Q89_B': 'sausage',\n    'Q89_C': 'dried meat products',\n    'Q89_D': 'pâté/meat spread',\n    'Q89_E': 'lamb',\n    'Q89_F': 'veal',\n    'Q89_G': 'goat',\n    'Q89_H': 'organ meats or offal',\n    'Q90': 'shawarma or donair',\n    'Q91_1': 'chicken shawarma/donair',\n    'Q91_2': 'beef shawarma/donair',\n    'Q91_3': 'pork shawarma/donair',\n    'Q91_4': 'lamb shawarma/donair',\n    'Q91_5': 'turkey shawarma/donair',\n    'Q91_6': 'veal shawarma/donair',\n    'Q91_7': 'mixed meats shawarma/donair',\n    'Q91_8': 'other shawarma/donair',\n    'QN2_A': 'caribou',\n    'QN2_B': 'geese',\n    'QN2_C': 'duck',\n    'QN2_D': 'muskox',\n    'QN2_E': 'polar bear',\n    'QN2_F': 'seal',\n    'QN2_G': 'walrus',\n    'QN2_H': 'beluga',\n    'QN2_I': 'narwhal',\n    'QN2_J': 'bowhead',\n    'QN2_K': 'ptarmigan/grouse',\n    'QN2_L': 'moose',\n    'QN2_M': 'sheep',\n    'QN2_N': 'bear',\n    'QN2_O': 'bison',\n    'QN2_P': 'elk/deer',\n    'QN2_Q': 'gophers',\n    'QN2_R': 'beaver/muskrat',\n    'QN2_S': 'rabbit',\n    'Q93': 'fish',\n    'Q94_A': 'smoked fish',\n    'Q94_B': 'raw fish',\n    'Q95': 'shellfish',\n    'Q96_A': 'mussels',\n    'Q96_B': 'clams',\n    'Q96_C': 'scallops',\n    'Q96_D': 'shrimps/prawns',\n    'Q96_E': 'crab',\n    'Q96_F': 'lobster',\n    'Q97': 'oysters',\n    'Q98': 'raw oysters',\n    'QN3_A': 'arctic char',\n    'QN3_B': 'whitefish',\n    'QN3_C': 'trout',\n    'QN3_D': 'herring',\n    'QN3_E': 'inconnu',\n    'QN3_F': 'salmon',\n    'QN3_G': 'cod',\n    'QN3_H': 'seaweed',\n    'QN3_I': 'pike',\n    'Q99': 'eggs',\n    'Q100': 'raw or undercooked eggs',\n    'QN4_A': 'duck eggs',\n    'QN4_B': 'geese eggs',\n    'QN4_C': 'other wild eggs',\n    'Q101': 'dairy products',\n    'Q102_A': 'pasteurized dairy milk',\n    'Q102_B': 'unpasteurized dairy milk',\n    'Q102_C': 'powdered milk product',\n    'Q102_D': 'whipped/whipping cream',\n    'Q102_E': 'sour cream',\n    'Q102_F': 'ice cream/gelato',\n    'Q102_G': 'yogurt',\n    'Q103': 'dairy substitutes or non-dairy milk',\n    'Q104': 'cheese products',\n    'Q105_A': 'cheddar cheese',\n    'Q105_B': 'mozzarella',\n    'Q105_C': 'parmesan cheese',\n    'Q105_D': 'gouda',\n    'Q105_E': 'feta cheese',\n    'Q105_F': 'other block cheeses',\n    'Q105_G': 'brie, camembert or soft cheeses',\n    'Q105_H': 'blue-veined cheese',\n    'Q105_I': 'cottage, ricotta or fresh cheese',\n    'Q105_J': 'goat/sheep milk cheese',\n    'Q105_K': 'processed cheese',\n    'Q105_L': 'raw milk cheese',\n    'Q106_A': 'frozen vegetables',\n    'Q106_B': 'frozen berries',\n    'Q106_C': 'frozen fruit other than berries',\n    'Q106_D': 'frozen pizza',\n    'Q106_E': 'frozen pot pies',\n    'Q106_F': 'frozen meals',\n    'Q106_G': 'frozen snack foods/appetizers',\n    'Q107_A': 'dried fruit',\n    'Q107_B': 'granola bars, power bars, or protein bars',\n    'Q107_C': 'chips or pretzels',\n    'Q107_D': 'chocolate or chocolate-containing candy',\n    'Q107_E': 'cold breakfast cereal',\n    'Q107_F': 'hot breakfast cereal',\n    'Q107_G': 'tofu',\n    'Q108': 'dietary or nutritional supplement'\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T00:42:39.019484Z","iopub.execute_input":"2025-11-24T00:42:39.020043Z","iopub.status.idle":"2025-11-24T00:42:39.033310Z","shell.execute_reply.started":"2025-11-24T00:42:39.020019Z","shell.execute_reply":"2025-11-24T00:42:39.032605Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import pandas as pd\n\n# Create DataFrame from the dictionary\ndf = pd.DataFrame(list(food_question_dict.items()), columns=['Question', 'Food_Item'])\n\n# Display the first few rows\nprint(df.head())\n\n# Save to CSV if needed\ndf.to_csv('food_question_mapping.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T00:43:11.006598Z","iopub.execute_input":"2025-11-24T00:43:11.006892Z","iopub.status.idle":"2025-11-24T00:43:11.014918Z","shell.execute_reply.started":"2025-11-24T00:43:11.006871Z","shell.execute_reply":"2025-11-24T00:43:11.014188Z"}},"outputs":[{"name":"stdout","text":"  Question                Food_Item\n0       Q6                 tomatoes\n1       Q9  lettuce or leafy greens\n2      Q13                  cabbage\n3      Q14                  sprouts\n4    Q15_A          alfalfa sprouts\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sentence_transformers import SentenceTransformer, util\nimport numpy as np\nimport time\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\n\nclass HybridProductCategorizer:\n    \"\"\"\n    Hybrid categorization combining semantic similarity with keyword matching\n    for low-confidence classifications.\n    \"\"\"\n    \n    def __init__(self, semantic_threshold=0.6, keyword_threshold=0.4):\n        self.semantic_threshold = semantic_threshold\n        self.keyword_threshold = keyword_threshold\n        self.device = self._setup_device()\n        self.model = None\n        self.food_embeddings = None\n        self.food_items = None\n        self.keyword_mappings = self._create_keyword_mappings()\n        \n    def _setup_device(self):\n        \"\"\"Initialize and configure computing device.\"\"\"\n        if torch.cuda.is_available():\n            gpu_count = torch.cuda.device_count()\n            print(f\"GPU Configuration: {gpu_count} GPU(s) detected\")\n            return torch.device('cuda')\n        else:\n            print(\"CPU Configuration: No GPU detected, using CPU\")\n            return torch.device('cpu')\n    \n    def initialize_model(self):\n        \"\"\"Load and initialize the sentence transformer model.\"\"\"\n        print(\"Initializing semantic model...\")\n        start_time = time.time()\n        self.model = SentenceTransformer('all-mpnet-base-v2', device=self.device)\n        load_time = time.time() - start_time\n        print(f\"Model initialized in {load_time:.2f} seconds\")\n        return \"all-mpnet-base-v2\"\n    \n    def _create_keyword_mappings(self):\n        \"\"\"Create comprehensive keyword to category mappings.\"\"\"\n        return {\n            # Fruits\n            r'\\bbanana': 'bananas',\n            r'\\bapple': 'apples',\n            r'\\borange': 'citrus fruit',\n            r'\\blemon': 'citrus fruit',\n            r'\\blime': 'citrus fruit',\n            r'\\bgrapefruit': 'citrus fruit',\n            r'\\bgrape': 'grapes',\n            r'\\bstrawberr': 'strawberries',\n            r'\\bblueberr': 'blueberries',\n            r'\\braspberr': 'raspberries',\n            r'\\bblackberr': 'blackberries',\n            r'\\bcherr': 'cherries',\n            r'\\bpeach': 'peaches',\n            r'\\bpear': 'pears',\n            r'\\bplum': 'plums',\n            r'\\bavocado': 'avocado',\n            r'\\btomato': 'tomatoes',\n            r'\\bwatermelon': 'watermelon',\n            r'\\bcantaloupe': 'cantaloupe',\n            r'\\bpineapple': 'pineapple',\n            r'\\bkiwi': 'kiwi',\n            r'\\bmango': 'mangos',\n            \n            # Vegetables\n            r'\\bspinach': 'spinach',\n            r'\\blettuce': 'lettuce or leafy greens',\n            r'\\bkale': 'lettuce or leafy greens',\n            r'\\bcarrot': 'carrots',\n            r'\\bbroccoli': 'broccoli',\n            r'\\bcauliflower': 'cauliflower',\n            r'\\bcucumber': 'cucumbers',\n            r'\\bpepper': 'bell peppers',\n            r'\\bonion': 'onions',\n            r'\\bgarlic': 'fresh garlic',\n            r'\\bmushroom': 'mushrooms',\n            r'\\bcelery': 'celery',\n            r'\\bzucchini': 'zucchini',\n            r'\\bcabbage': 'cabbage',\n            r'\\bbean': 'green or yellow beans',\n            r'\\bpea': 'peas',\n            r'\\bsprout': 'sprouts',\n            \n            # Dairy & Alternatives\n            r'\\bmilk\\b': 'pasteurized dairy milk',\n            r'\\byogurt': 'yogurt',\n            r'\\bcheese': 'cheese products',\n            r'\\bcream': 'whipped/whipping cream',\n            r'\\balmond milk': 'dairy substitutes or non-dairy milk',\n            r'\\bsoy milk': 'dairy substitutes or non-dairy milk',\n            r'\\bcoconut milk': 'dairy substitutes or non-dairy milk',\n            r'\\boat milk': 'dairy substitutes or non-dairy milk',\n            \n            # Meat & Protein\n            r'\\bchicken': 'chicken',\n            r'\\bbeef': 'beef',\n            r'\\bpork': 'pork',\n            r'\\bturkey': 'turkey',\n            r'\\bfish': 'fish',\n            r'\\bsalmon': 'fish',\n            r'\\btuna': 'fish',\n            r'\\bshrimp': 'shrimps/prawns',\n            r'\\bbacon': 'bacon',\n            r'\\bsausage': 'sausage',\n            r'\\bham': 'ham',\n            r'\\begg': 'eggs',\n            \n            # Other\n            r'\\bchips': 'chips or pretzels',\n            r'\\bpretzel': 'chips or pretzels',\n            r'\\bchocolate': 'chocolate or chocolate-containing candy',\n            r'\\bnut\\b': 'nuts',\n            r'\\bseed': 'sunflower seeds',\n            r'\\bpeanut': 'peanuts',\n            r'\\balmond': 'almonds',\n            r'\\bwalnut': 'walnuts',\n            r'\\bcashew': 'cashews',\n            r'\\bpecan': 'pecans',\n        }\n    \n    def create_enhanced_category_descriptions(self, food_question_df):\n        \"\"\"Create enhanced category descriptions for semantic matching.\"\"\"\n        print(\"Creating enhanced category descriptions...\")\n        \n        enhanced_descriptions = {\n            'pasteurized dairy milk': 'milk dairy beverage drink whole reduced fat 2% 1% skim vitamin d',\n            'dairy substitutes or non-dairy milk': 'almond milk soy milk coconut milk oat milk rice milk non-dairy plant-based alternative',\n            'yogurt': 'yogurt greek yogurt dairy cultured probiotic',\n            'cheese products': 'cheese dairy cheddar mozzarella swiss provolone',\n            'bananas': 'bananas fruit yellow ripe',\n            'spinach': 'spinach leafy greens vegetable green',\n            'tomatoes': 'tomatoes fruit vegetable red',\n            'apples': 'apples fruit red green',\n            'citrus fruit': 'oranges lemons limes grapefruit citrus',\n            'strawberries': 'strawberries berries fruit red',\n        }\n        \n        self.food_items = food_question_df['Food_Item'].unique().tolist()\n        enhanced_categories = []\n        \n        for category in self.food_items:\n            if category in enhanced_descriptions:\n                enhanced_categories.append(enhanced_descriptions[category])\n            else:\n                enhanced_categories.append(category)\n        \n        print(f\"Created enhanced descriptions for {len(enhanced_categories)} categories\")\n        return enhanced_categories\n    \n    def keyword_categorization(self, product_name):\n        \"\"\"Categorize product using keyword matching.\"\"\"\n        if pd.isna(product_name):\n            return None, 0.0\n        \n        product_lower = product_name.lower()\n        best_match = None\n        best_confidence = 0.0\n        \n        for pattern, category in self.keyword_mappings.items():\n            if re.search(pattern, product_lower):\n                # Calculate confidence based on match quality\n                confidence = 0.7  # Base confidence for keyword matches\n                \n                # Boost confidence for exact matches\n                if pattern.replace(r'\\b', '') in product_lower.split():\n                    confidence = 0.9\n                \n                if confidence > best_confidence:\n                    best_match = category\n                    best_confidence = confidence\n        \n        return best_match, best_confidence\n    \n    def semantic_categorization(self, product_names):\n        \"\"\"Categorize products using semantic similarity.\"\"\"\n        if isinstance(product_names, str):\n            product_names = [product_names]\n        \n        try:\n            product_embeddings = self.model.encode(\n                product_names, \n                convert_to_tensor=True, \n                device=self.device,\n                show_progress_bar=False\n            )\n            \n            similarities = util.cos_sim(product_embeddings, self.food_embeddings)\n            max_similarities, max_indices = torch.max(similarities, dim=1)\n            \n            categories = []\n            confidences = []\n            \n            for i, similarity in enumerate(max_similarities):\n                if similarity > self.semantic_threshold:\n                    categories.append(self.food_items[max_indices[i]])\n                    confidences.append(float(similarity))\n                else:\n                    categories.append(None)  # Mark for keyword fallback\n                    confidences.append(float(similarity))\n            \n            return categories, confidences\n            \n        except Exception as e:\n            print(f\"Error in semantic categorization: {e}\")\n            return [None] * len(product_names), [0.0] * len(product_names)\n    \n    def hybrid_categorization(self, product_names):\n        \"\"\"Hybrid categorization combining semantic and keyword approaches.\"\"\"\n        if isinstance(product_names, str):\n            product_names = [product_names]\n        \n        # First pass: semantic categorization\n        semantic_categories, semantic_confidences = self.semantic_categorization(product_names)\n        \n        final_categories = []\n        final_confidences = []\n        method_used = []  # Track which method was used\n        \n        for i, (product, semantic_cat, semantic_conf) in enumerate(zip(product_names, semantic_categories, semantic_confidences)):\n            \n            # If semantic confidence is high, use it\n            if semantic_conf > self.semantic_threshold:\n                final_categories.append(semantic_cat)\n                final_confidences.append(semantic_conf)\n                method_used.append(\"semantic\")\n                \n            else:\n                # Fall back to keyword matching\n                keyword_cat, keyword_conf = self.keyword_categorization(product)\n                \n                if keyword_cat and keyword_conf > self.keyword_threshold:\n                    final_categories.append(keyword_cat)\n                    final_confidences.append(keyword_conf)\n                    method_used.append(\"keyword\")\n                else:\n                    final_categories.append(\"ignore\")\n                    final_confidences.append(max(semantic_conf, keyword_conf))\n                    method_used.append(\"ignore\")\n        \n        return final_categories, final_confidences, method_used\n    \n    def categorize_all_products_hybrid(self, final_df, food_question_df):\n        \"\"\"\n        Categorize all products using hybrid approach.\n        \"\"\"\n        total_start_time = time.time()\n        \n        print(\"HYBRID CATEGORIZATION PIPELINE\")\n        print(\"=\" * 70)\n        \n        # Initialize model\n        model_name = self.initialize_model()\n        enhanced_categories = self.create_enhanced_category_descriptions(food_question_df)\n        \n        # Precompute category embeddings\n        print(\"Computing category embeddings...\")\n        start_time = time.time()\n        self.food_embeddings = self.model.encode(\n            enhanced_categories, \n            convert_to_tensor=True, \n            device=self.device,\n            show_progress_bar=True\n        )\n        embed_time = time.time() - start_time\n        print(f\"Category embeddings computed in {embed_time:.2f} seconds\")\n        \n        # Analyze input data\n        print(\"\\nDATA ANALYSIS\")\n        print(\"-\" * 40)\n        unique_products = final_df['product_name'].dropna().unique()\n        total_unique_products = len(unique_products)\n        total_dataframe_rows = len(final_df)\n        \n        print(f\"Unique products to categorize: {total_unique_products:,}\")\n        print(f\"Total dataframe rows: {total_dataframe_rows:,}\")\n        print(f\"Semantic threshold: {self.semantic_threshold}\")\n        print(f\"Keyword threshold: {self.keyword_threshold}\")\n        \n        # Process all products with hybrid approach\n        print(\"\\nHYBRID PROCESSING\")\n        print(\"-\" * 50)\n        \n        processing_start_time = time.time()\n        product_to_category = {}\n        product_confidence = {}\n        product_method = {}\n        \n        batch_size = 1000\n        total_batches = (total_unique_products + batch_size - 1) // batch_size\n        \n        semantic_count = 0\n        keyword_count = 0\n        ignore_count = 0\n        \n        for batch_idx in range(0, total_unique_products, batch_size):\n            batch_num = (batch_idx // batch_size) + 1\n            batch_products = unique_products[batch_idx:batch_idx + batch_size]\n            \n            if batch_num % 10 == 0 or batch_num <= 3:\n                print(f\"Processing batch {batch_num}/{total_batches} ({len(batch_products):,} products)\")\n            \n            batch_start_time = time.time()\n            \n            # Categorize batch with hybrid approach\n            batch_categories, batch_confidences, batch_methods = self.hybrid_categorization(batch_products)\n            \n            # Update mappings and count methods\n            for product, category, confidence, method in zip(batch_products, batch_categories, batch_confidences, batch_methods):\n                product_to_category[product] = category\n                product_confidence[product] = confidence\n                product_method[product] = method\n                \n                if method == \"semantic\":\n                    semantic_count += 1\n                elif method == \"keyword\":\n                    keyword_count += 1\n                else:\n                    ignore_count += 1\n            \n            batch_time = time.time() - batch_start_time\n            \n            # Show samples from first batch\n            if batch_num == 1:\n                print(f\"\\n  HYBRID SAMPLE CATEGORIZATIONS (Batch 1):\")\n                print(\"  \" + \"-\" * 80)\n                sample_products = [\n                    \"Whole Vitamin D Milk\",\n                    \"Bag of Organic Bananas\", \n                    \"Organic Baby Spinach\",\n                    \"Almond Milk\",\n                    \"Fresh Spinach\",\n                    \"Greek Yogurt\"\n                ]\n                for test_product in sample_products:\n                    if test_product in product_to_category:\n                        category = product_to_category[test_product]\n                        confidence = product_confidence[test_product]\n                        method = product_method[test_product]\n                        status = \"✓\" if category != \"ignore\" else \"✗\"\n                        print(f\"    {status} [{method:<8}] {test_product:<35} -> {category:<25} (conf: {confidence:.3f})\")\n        \n        processing_time = time.time() - processing_start_time\n        print(f\"\\nHybrid processing completed in {processing_time:.2f} seconds\")\n        print(f\"Method distribution: Semantic={semantic_count:,} Keyword={keyword_count:,} Ignore={ignore_count:,}\")\n        \n        # Apply to dataframe\n        print(\"\\nAPPLYING HYBRID PREDICTIONS TO DATAFRAME\")\n        print(\"-\" * 50)\n        \n        integration_start_time = time.time()\n        categorized_df = final_df.copy()\n        categorized_df['food_category'] = categorized_df['product_name'].map(product_to_category)\n        categorized_df['food_category'] = categorized_df['food_category'].fillna('ignore')\n        categorized_df['confidence'] = categorized_df['product_name'].map(product_confidence)\n        categorized_df['confidence'] = categorized_df['confidence'].fillna(0.0)\n        categorized_df['method'] = categorized_df['product_name'].map(product_method)\n        categorized_df['method'] = categorized_df['method'].fillna('ignore')\n        \n        integration_time = time.time() - integration_start_time\n        print(f\"Dataframe integration completed in {integration_time:.2f} seconds\")\n        \n        # Generate statistics\n        self._generate_hybrid_statistics(categorized_df, total_dataframe_rows, total_start_time, semantic_count, keyword_count, ignore_count)\n        \n        # Create downloadable mapping\n        mapping_df = self._create_mapping_dataframe(product_to_category, product_confidence, product_method)\n        \n        return categorized_df, mapping_df\n    \n    def _generate_hybrid_statistics(self, categorized_df, total_rows, start_time, semantic_count, keyword_count, ignore_count):\n        \"\"\"Generate comprehensive statistics.\"\"\"\n        total_time = time.time() - start_time\n        category_counts = categorized_df['food_category'].value_counts()\n        ignore_count_total = category_counts.get('ignore', 0)\n        categorized_count = total_rows - ignore_count_total\n        \n        method_counts = categorized_df['method'].value_counts()\n        confidence_stats = categorized_df[categorized_df['food_category'] != 'ignore']['confidence'].describe()\n        \n        print(\"\\n\" + \"=\" * 70)\n        print(\"HYBRID CATEGORIZATION COMPLETED\")\n        print(\"=\" * 70)\n        print(f\"Total processing time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n        print(f\"Total rows processed: {total_rows:,}\")\n        print(f\"Products categorized: {categorized_count:,} ({categorized_count/total_rows*100:.1f}%)\")\n        print(f\"Products ignored: {ignore_count_total:,} ({ignore_count_total/total_rows*100:.1f}%)\")\n        \n        print(f\"\\nMETHOD DISTRIBUTION:\")\n        print(f\"  Semantic: {semantic_count:,} ({semantic_count/total_rows*100:.1f}%)\")\n        print(f\"  Keyword:  {keyword_count:,} ({keyword_count/total_rows*100:.1f}%)\")\n        print(f\"  Ignore:   {ignore_count:,} ({ignore_count/total_rows*100:.1f}%)\")\n        \n        print(f\"\\nCONFIDENCE STATISTICS:\")\n        print(f\"  Mean: {confidence_stats['mean']:.3f}\")\n        print(f\"  Std:  {confidence_stats['std']:.3f}\")\n        \n        print(f\"\\nTOP 20 CATEGORIES:\")\n        print(\"-\" * 60)\n        for category, count in category_counts.head(20).items():\n            percentage = count / total_rows * 100\n            print(f\"  {category:<45} {count:>8,} ({percentage:5.1f}%)\")\n    \n    def _create_mapping_dataframe(self, product_to_category, product_confidence, product_method):\n        \"\"\"Create a downloadable dataframe with product to category mappings.\"\"\"\n        print(\"\\nCREATING MAPPING DATAFRAME...\")\n        \n        mapping_data = []\n        for product, category in product_to_category.items():\n            mapping_data.append({\n                'product_name': product,\n                'food_category': category,\n                'confidence': product_confidence.get(product, 0.0),\n                'method': product_method.get(product, 'unknown')\n            })\n        \n        mapping_df = pd.DataFrame(mapping_data)\n        \n        # Sort by confidence for easier analysis\n        mapping_df = mapping_df.sort_values('confidence', ascending=False)\n        \n        print(f\"Created mapping dataframe with {len(mapping_df):,} unique products\")\n        print(f\"Sample of high-confidence mappings:\")\n        high_conf_samples = mapping_df[mapping_df['confidence'] > 0.8].head(5)\n        for _, row in high_conf_samples.iterrows():\n            print(f\"  ✓ {row['product_name'][:50]:<50} -> {row['food_category']} (conf: {row['confidence']:.3f})\")\n        \n        return mapping_df\n\n\ndef main_hybrid_categorization():\n    \"\"\"Main execution with hybrid categorization.\"\"\"\n    try:\n        print(\"Loading datasets for hybrid categorization...\")\n        \n        final_df = pd.read_csv('user_orders_products.csv')\n        food_question_df = pd.DataFrame(list(food_question_dict.items()), \n                                      columns=['Question', 'Food_Item'])\n        \n        print(f\"Final dataframe loaded: {len(final_df):,} rows\")\n        \n        # Initialize hybrid categorizer with optimized thresholds\n        categorizer = HybridProductCategorizer(\n            semantic_threshold=0.6,\n            keyword_threshold=0.5\n        )\n        \n        # Execute hybrid categorization\n        categorized_results, mapping_df = categorizer.categorize_all_products_hybrid(\n            final_df, food_question_df\n        )\n        \n        # Save results\n        print(\"\\nSAVING RESULTS...\")\n        save_start = time.time()\n        \n        # Save the main categorized dataframe\n        categorized_results.to_csv('user_orders_products_categorized_hybrid.csv', index=False)\n        print(\"✓ Main categorized data saved: user_orders_products_categorized_hybrid.csv\")\n        \n        # Save the product-category mapping dataframe\n        mapping_df.to_csv('product_category_mapping.csv', index=False)\n        print(\"✓ Product-category mapping saved: product_category_mapping.csv\")\n        \n        # Save a sample for quick review\n        sample_mapping = mapping_df.head(1000)\n        sample_mapping.to_csv('product_category_mapping_sample.csv', index=False)\n        print(\"✓ Sample mapping saved: product_category_mapping_sample.csv\")\n        \n        save_time = time.time() - save_start\n        print(f\"All files saved in {save_time:.2f} seconds\")\n        \n        print(\"\\n\" + \"=\" * 70)\n        print(\"HYBRID CATEGORIZATION COMPLETED SUCCESSFULLY\")\n        print(\"=\" * 70)\n        print(\"Files created:\")\n        print(\"  1. user_orders_products_categorized_hybrid.csv - Full dataset with categories\")\n        print(\"  2. product_category_mapping.csv - Product to category mappings\")\n        print(\"  3. product_category_mapping_sample.csv - Sample of mappings for review\")\n        \n        return categorized_results, mapping_df\n        \n    except Exception as e:\n        print(f\"ERROR in hybrid categorization: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n\n\nif __name__ == \"__main__\":\n    hybrid_results, mapping_df = main_hybrid_categorization()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T01:22:22.896112Z","iopub.execute_input":"2025-11-24T01:22:22.896759Z","iopub.status.idle":"2025-11-24T01:25:56.454110Z","shell.execute_reply.started":"2025-11-24T01:22:22.896733Z","shell.execute_reply":"2025-11-24T01:25:56.453252Z"}},"outputs":[{"name":"stdout","text":"Loading datasets for hybrid categorization...\nFinal dataframe loaded: 33,819,106 rows\nGPU Configuration: 2 GPU(s) detected\nHYBRID CATEGORIZATION PIPELINE\n======================================================================\nInitializing semantic model...\nModel initialized in 1.14 seconds\nCreating enhanced category descriptions...\nCreated enhanced descriptions for 221 categories\nComputing category embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eac243f97f35434d97766f9242d9e31a"}},"metadata":{}},{"name":"stdout","text":"Category embeddings computed in 0.19 seconds\n\nDATA ANALYSIS\n----------------------------------------\nUnique products to categorize: 49,685\nTotal dataframe rows: 33,819,106\nSemantic threshold: 0.6\nKeyword threshold: 0.5\n\nHYBRID PROCESSING\n--------------------------------------------------\nProcessing batch 1/50 (1,000 products)\n\n  HYBRID SAMPLE CATEGORIZATIONS (Batch 1):\n  --------------------------------------------------------------------------------\n    ✓ [semantic] Whole Vitamin D Milk                -> pasteurized dairy milk    (conf: 0.702)\n    ✓ [keyword ] Bag of Organic Bananas              -> bananas                   (conf: 0.700)\n    ✓ [keyword ] Organic Baby Spinach                -> spinach                   (conf: 0.900)\nProcessing batch 2/50 (1,000 products)\nProcessing batch 3/50 (1,000 products)\nProcessing batch 10/50 (1,000 products)\nProcessing batch 20/50 (1,000 products)\nProcessing batch 30/50 (1,000 products)\nProcessing batch 40/50 (1,000 products)\nProcessing batch 50/50 (685 products)\n\nHybrid processing completed in 51.57 seconds\nMethod distribution: Semantic=10,526 Keyword=15,046 Ignore=24,113\n\nAPPLYING HYBRID PREDICTIONS TO DATAFRAME\n--------------------------------------------------\nDataframe integration completed in 16.04 seconds\n\n======================================================================\nHYBRID CATEGORIZATION COMPLETED\n======================================================================\nTotal processing time: 74.71 seconds (1.25 minutes)\nTotal rows processed: 33,819,106\nProducts categorized: 24,432,499 (72.2%)\nProducts ignored: 9,386,607 (27.8%)\n\nMETHOD DISTRIBUTION:\n  Semantic: 10,526 (0.0%)\n  Keyword:  15,046 (0.0%)\n  Ignore:   24,113 (0.1%)\n\nCONFIDENCE STATISTICS:\n  Mean: 0.758\n  Std:  0.112\n\nTOP 20 CATEGORIES:\n------------------------------------------------------------\n  ignore                                        9,386,607 ( 27.8%)\n  citrus fruit                                  1,230,706 (  3.6%)\n  yogurt                                        1,126,808 (  3.3%)\n  bananas                                       1,009,571 (  3.0%)\n  apples                                         870,544 (  2.6%)\n  pasteurized dairy milk                         794,718 (  2.3%)\n  tomatoes                                       684,209 (  2.0%)\n  chips or pretzels                              645,477 (  1.9%)\n  avocado                                        595,755 (  1.8%)\n  chicken                                        545,081 (  1.6%)\n  chocolate or chocolate-containing candy        530,003 (  1.6%)\n  unpasteurized dairy milk                       525,297 (  1.6%)\n  strawberries                                   506,898 (  1.5%)\n  raspberries                                    485,485 (  1.4%)\n  cheese products                                473,931 (  1.4%)\n  bell peppers                                   465,336 (  1.4%)\n  lettuce or leafy greens                        433,744 (  1.3%)\n  almonds                                        430,553 (  1.3%)\n  green or yellow beans                          428,757 (  1.3%)\n  spinach                                        403,002 (  1.2%)\n\nCREATING MAPPING DATAFRAME...\nCreated mapping dataframe with 49,685 unique products\nSample of high-confidence mappings:\n  ✓ Apricots                                           -> apricots (conf: 1.000)\n  ✓ Fish                                               -> fish (conf: 1.000)\n  ✓ Gouda                                              -> gouda (conf: 1.000)\n  ✓ Peanut Butter                                      -> peanut butter (conf: 1.000)\n  ✓ Chicken                                            -> chicken (conf: 1.000)\n\nSAVING RESULTS...\n✓ Main categorized data saved: user_orders_products_categorized_hybrid.csv\n✓ Product-category mapping saved: product_category_mapping.csv\n✓ Sample mapping saved: product_category_mapping_sample.csv\nAll files saved in 114.79 seconds\n\n======================================================================\nHYBRID CATEGORIZATION COMPLETED SUCCESSFULLY\n======================================================================\nFiles created:\n  1. user_orders_products_categorized_hybrid.csv - Full dataset with categories\n  2. product_category_mapping.csv - Product to category mappings\n  3. product_category_mapping_sample.csv - Sample of mappings for review\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"!rm -rf /kaggle/working/test1.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T01:26:20.349957Z","iopub.execute_input":"2025-11-24T01:26:20.350224Z","iopub.status.idle":"2025-11-24T01:26:20.807606Z","shell.execute_reply.started":"2025-11-24T01:26:20.350205Z","shell.execute_reply":"2025-11-24T01:26:20.806540Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import pandas as pd\n\n# Load the datasets\nprint(\"Loading datasets...\")\noriginal_df = pd.read_csv('user_orders_products.csv')\nmapping_df = pd.read_csv('product_category_mapping.csv')\n\nprint(f\"Original dataset: {len(original_df):,} rows\")\nprint(f\"Mapping dataset: {len(mapping_df):,} unique products\")\n\n# Merge to add food_category\nprint(\"\\nMerging datasets...\")\nfinal_df = original_df.merge(\n    mapping_df[['product_name', 'food_category']],\n    on='product_name',\n    how='left'\n)\n\n# Fill any missing categories with 'ignore'\nfinal_df['food_category'] = final_df['food_category'].fillna('ignore')\n\n# Select only the required columns\nfinal_df = final_df[['user_id', 'order_no', 'food_category']]\n\n# Display the head\nprint(\"\\nFirst 10 rows of the final dataset:\")\nprint(final_df.head(10))\n\n# Save the final file\nprint(\"\\nSaving final file...\")\nfinal_df.to_csv('user_orders_product_food_category.csv', index=False)\nprint(\"✓ File saved: user_orders_product_food_category.csv\")\n\n# Show some statistics\nprint(f\"\\nDataset statistics:\")\nprint(f\"Total rows: {len(final_df):,}\")\nprint(f\"Unique users: {final_df['user_id'].nunique():,}\")\nprint(f\"Unique food categories: {final_df['food_category'].nunique()}\")\nprint(f\"Most common categories:\")\nprint(final_df['food_category'].value_counts().head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T01:34:33.767782Z","iopub.execute_input":"2025-11-24T01:34:33.768491Z","iopub.status.idle":"2025-11-24T01:35:40.597327Z","shell.execute_reply.started":"2025-11-24T01:34:33.768421Z","shell.execute_reply":"2025-11-24T01:35:40.596667Z"}},"outputs":[{"name":"stdout","text":"Loading datasets...\nOriginal dataset: 33,819,106 rows\nMapping dataset: 49,685 unique products\n\nMerging datasets...\n\nFirst 10 rows of the final dataset:\n   user_id  order_no                        food_category\n0        1         1                               ignore\n1        1         1  dairy substitutes or non-dairy milk\n2        1         1                       beef deli meat\n3        1         1                               ignore\n4        1         1                               ignore\n5        1         2                               ignore\n6        1         2                              almonds\n7        1         2                       beef deli meat\n8        1         2                              bananas\n9        1         2                               ignore\n\nSaving final file...\n✓ File saved: user_orders_product_food_category.csv\n\nDataset statistics:\nTotal rows: 33,819,106\nUnique users: 206,209\nUnique food categories: 196\nMost common categories:\nfood_category\nignore                    9386607\ncitrus fruit              1230706\nyogurt                    1126808\nbananas                   1009571\napples                     870544\npasteurized dairy milk     794718\ntomatoes                   684209\nchips or pretzels          645477\navocado                    595755\nchicken                    545081\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"!rm -rf /kaggle/working/user_orders_product_food_category","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom collections import Counter\n\nprint(\"NEXT BASKET PREDICTION - FIXED MULTI-LABEL APPROACH\")\nprint(\"=\" * 60)\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load and process data\ndf1 = pd.read_csv('/kaggle/input/simplifiedinstacartdata/all_order_products.csv')\ndf2 = pd.read_csv('/kaggle/input/simplifiedinstacartdata/orders.csv')\ndf = df2.merge(df1, on='order_id')\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf = df.sort_values(['user_id', 'timestamp', 'add_to_cart_order'])\n\n# Filter users with at least 4 orders\nuser_order_counts = df.groupby('user_id')['order_id'].nunique()\nvalid_users = user_order_counts[user_order_counts >= 4].index\ndf = df[df['user_id'].isin(valid_users)]\n\n# Add order sequence and keep first 4\ndf['order_seq'] = df.groupby('user_id').cumcount() + 1\ndf = df[df['order_seq'] <= 4]\n\n\n\n\n\n# Take 30% of users\nall_users_with_4_orders = df['user_id'].unique()\nselected_users = np.random.choice(all_users_with_4_orders, \n                                 size=int(len(all_users_with_4_orders) * 0.3), \n                                 replace=False)\ndf = df[df['user_id'].isin(selected_users)]\n\nprint(f\"Working with {len(selected_users):,} users\")\n\n# Create sequences\nuser_sequences = {}\nfor user_id in df['user_id'].unique():\n    user_data = df[df['user_id'] == user_id]\n    orders = {}\n    for seq_num in range(1, 5):\n        order_products = user_data[user_data['order_seq'] == seq_num]['product_id'].tolist()\n        orders[seq_num] = order_products\n    if len(orders) == 4:\n        user_sequences[user_id] = {\n            'input': [orders[1], orders[2], orders[3]],\n            'target': orders[4]\n        }\n\nprint(f\"Created {len(user_sequences):,} sequences\")\n\n# Split users\nuser_ids = list(user_sequences.keys())\ntrain_users, test_users = train_test_split(user_ids, test_size=0.3, random_state=42)\ntrain_sequences = [user_sequences[uid] for uid in train_users]\ntest_sequences = [user_sequences[uid] for uid in test_users]\n\nprint(f\"Train: {len(train_sequences):,}, Test: {len(test_sequences):,}\")\n\n# FIX: Build vocabulary using ALL products from both train and test\nprint(\"\\nBUILDING COMPLETE VOCABULARY...\")\nall_products = set()\nfor user_id in user_sequences:\n    for basket in user_sequences[user_id]['input']:\n        all_products.update(basket)\n    all_products.update(user_sequences[user_id]['target'])\n\nproduct_to_idx = {product: idx+1 for idx, product in enumerate(all_products)}\nidx_to_product = {idx: product for product, idx in product_to_idx.items()}\nvocab_size = len(product_to_idx) + 1\n\nprint(f\"Complete vocabulary: {vocab_size:,} products\")\n\n# FIX: Debug - check vocabulary coverage\nprint(\"\\nCHECKING VOCABULARY COVERAGE:\")\ntotal_target_products = 0\ncovered_target_products = 0\n\nfor seq in train_sequences + test_sequences:\n    for product in seq['target']:\n        total_target_products += 1\n        if product in product_to_idx:\n            covered_target_products += 1\n\ncoverage = covered_target_products / total_target_products * 100\nprint(f\"Vocabulary covers {covered_target_products:,} out of {total_target_products:,} target products\")\nprint(f\"Coverage: {coverage:.1f}%\")\n\n# Multi-label Dataset - predicts entire basket\nclass BasketDataset(Dataset):\n    def __init__(self, sequences, product_to_idx, max_basket_size=30):\n        self.sequences = sequences\n        self.product_to_idx = product_to_idx\n        self.max_basket_size = max_basket_size\n        self.vocab_size = len(product_to_idx) + 1\n        \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        sequence = self.sequences[idx]\n        \n        # Input: first 3 orders as baskets\n        input_orders = []\n        for basket in sequence['input']:\n            basket_idx = []\n            for p in basket[:self.max_basket_size]:\n                if p in self.product_to_idx:\n                    basket_idx.append(self.product_to_idx[p])\n            padded_basket = basket_idx + [0] * (self.max_basket_size - len(basket_idx))\n            input_orders.append(padded_basket)\n        \n        input_tensor = torch.tensor(input_orders, dtype=torch.long)\n        \n        # Target: 4th order as multi-hot encoding\n        target = torch.zeros(self.vocab_size)\n        for product in sequence['target']:\n            if product in self.product_to_idx:\n                target[self.product_to_idx[product]] = 1.0\n        \n        return input_tensor, target\n\n# Create datasets\nmax_basket_size = 25\ntrain_dataset = BasketDataset(train_sequences, product_to_idx, max_basket_size)\ntest_dataset = BasketDataset(test_sequences, product_to_idx, max_basket_size)\n\nprint(f\"Training samples: {len(train_dataset):,}\")\n\n# FIX: Improved Basket Prediction Model\nclass ImprovedBasketPredictor(nn.Module):\n    def __init__(self, vocab_size, d_model=256, max_basket_size=25):\n        super(ImprovedBasketPredictor, self).__init__()\n        \n        self.product_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        \n        # Enhanced transformer architecture\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=8,\n            dim_feedforward=1024,\n            dropout=0.3,\n            batch_first=True,\n            activation='relu'\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n        \n        # Positional encoding\n        self.pos_encoding = nn.Parameter(torch.zeros(1, 3, d_model))\n        \n        # Enhanced output layers with residual connections\n        self.output_layers = nn.Sequential(\n            nn.Linear(d_model, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, vocab_size)\n        )\n        \n        # Initialize weights properly\n        self._init_weights()\n        \n    def _init_weights(self):\n        # Initialize embedding layer\n        nn.init.normal_(self.product_embedding.weight, mean=0.0, std=0.02)\n        # Initialize output layer with smaller weights\n        for layer in self.output_layers:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)\n                if layer.bias is not None:\n                    nn.init.zeros_(layer.bias)\n        \n    def forward(self, x):\n        # x shape: (batch_size, 3, max_basket_size)\n        batch_size, num_baskets, basket_size = x.shape\n        \n        # Embed all products in all baskets\n        embedded = self.product_embedding(x)  # (batch, 3, basket_size, d_model)\n        \n        # Average products in each basket to get basket representations\n        basket_reps = embedded.mean(dim=2)  # (batch, 3, d_model)\n        \n        # Add positional encoding and process with transformer\n        basket_reps = basket_reps + self.pos_encoding\n        transformed = self.transformer(basket_reps)  # (batch, 3, d_model)\n        \n        # Use the last basket's representation to predict next basket\n        last_basket_rep = transformed[:, -1, :]  # (batch, d_model)\n        \n        # Output probabilities for each product\n        output = self.output_layers(last_basket_rep)  # (batch, vocab_size)\n        \n        return output\n\nprint(\"\\nInitializing improved model...\")\nmodel = ImprovedBasketPredictor(vocab_size, max_basket_size=max_basket_size)\nmodel.to(device)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# FIX: Improved training setup with class weighting\n# Calculate class weights to handle imbalance\nprint(\"\\nCALCULATING CLASS WEIGHTS...\")\nproduct_frequencies = torch.zeros(vocab_size)\nfor _, target in train_dataset:\n    product_frequencies += target\n\n# Avoid division by zero for products that never appear\nproduct_frequencies = torch.clamp(product_frequencies, min=1)\nclass_weights = 1.0 / torch.log(1.0 + product_frequencies)\nclass_weights = class_weights.to(device)\n\nprint(f\"Class weights calculated for {vocab_size} products\")\n\n# Multi-label training setup with weighted loss\ncriterion = nn.BCEWithLogitsLoss(weight=class_weights, reduction='mean')\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\ndef calculate_basket_accuracy(model, loader, k=10):\n    \"\"\"Calculate how many products in the actual basket appear in top-k predictions\"\"\"\n    model.eval()\n    total_recall = 0\n    total_precision = 0\n    total_f1 = 0\n    num_baskets = 0\n    \n    with torch.no_grad():\n        for data, target in loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            probabilities = torch.sigmoid(output)\n            \n            # For each basket, get top-k predicted products\n            _, topk_indices = torch.topk(probabilities, k, dim=1)\n            \n            # For each sample, calculate metrics\n            for i in range(len(target)):\n                # Actual products in basket\n                actual_indices = set((target[i] == 1).nonzero(as_tuple=True)[0].cpu().numpy())\n                \n                # Skip if no actual products (shouldn't happen with complete vocabulary)\n                if len(actual_indices) == 0:\n                    continue\n                \n                # Predicted products (top-k)\n                predicted_indices = set(topk_indices[i].cpu().numpy())\n                \n                # Calculate recall: how many actual products were predicted\n                recall = len(actual_indices & predicted_indices) / len(actual_indices)\n                total_recall += recall\n                \n                # Calculate precision: how many predicted products were actual\n                precision = len(actual_indices & predicted_indices) / k\n                total_precision += precision\n                \n                # F1 score\n                if recall + precision > 0:\n                    f1 = 2 * (recall * precision) / (recall + precision)\n                    total_f1 += f1\n                \n                num_baskets += 1\n    \n    if num_baskets == 0:\n        return 0, 0, 0\n    \n    avg_recall = total_recall / num_baskets * 100\n    avg_precision = total_precision / num_baskets * 100  \n    avg_f1 = total_f1 / num_baskets * 100\n    \n    return avg_recall, avg_precision, avg_f1\n\ndef train_epoch():\n    model.train()\n    total_loss = 0\n    for data, target in train_loader:\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(train_loader)\n\nprint(\"\\nStarting training...\")\nprint(\"Epoch | Train Loss | Recall@10 | Precision@10 | F1@10\")\nprint(\"-\" * 55)\n\nbest_f1 = 0\nfor epoch in range(1, 51):\n    train_loss = train_epoch()\n    scheduler.step()\n    \n    if epoch % 5 == 0 or epoch <= 5:\n        recall, precision, f1 = calculate_basket_accuracy(model, test_loader, k=10)\n        current_lr = scheduler.get_last_lr()[0]\n        print(f\"{epoch:5d} | {train_loss:.4f}    | {recall:.2f}%     | {precision:.2f}%       | {f1:.2f}% | LR: {current_lr:.6f}\")\n        \n        if f1 > best_f1 and epoch >= 5:  # Only save after some training\n            best_f1 = f1\n            torch.save(model.state_dict(), 'best_improved_model.pth')\n            print(f\"  -> Saved model with F1: {f1:.2f}%\")\n\n# Final evaluation with different k values\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL EVALUATION:\")\nprint(\"=\"*50)\n\nmodel.load_state_dict(torch.load('best_improved_model.pth'))\nmodel.eval()\n\nfor k in [5, 10, 15, 20]:\n    recall, precision, f1 = calculate_basket_accuracy(model, test_loader, k=k)\n    print(f\"Top-{k:2d}: Recall: {recall:.2f}% | Precision: {precision:.2f}% | F1: {f1:.2f}%\")\n\nprint(f\"\\nBest F1 score: {best_f1:.2f}%\")\n\n# Multiple sample predictions\nprint(\"\\n\" + \"=\"*50)\nprint(\"SAMPLE PREDICTIONS:\")\nprint(\"=\"*50)\n\nmodel.eval()\nfor sample_idx in range(3):  # Show 3 samples\n    sample_input, sample_target = test_dataset[sample_idx]\n    sample_input = sample_input.unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        output = model(sample_input)\n        probabilities = torch.sigmoid(output)\n        top_probs, top_indices = torch.topk(probabilities, 15)\n\n    predicted_products = [idx_to_product[idx.item()] for idx in top_indices[0] if idx.item() in idx_to_product]\n    actual_products = [idx_to_product[idx] for idx in range(len(sample_target)) if sample_target[idx] == 1]\n\n    print(f\"\\nSample {sample_idx + 1}:\")\n    print(f\"Predicted top 15: {predicted_products}\")\n    print(f\"Actual basket: {actual_products}\")\n    \n    # Calculate overlaps for different k values\n    for k in [5, 10, 15]:\n        predicted_k = set(predicted_products[:k])\n        actual_set = set(actual_products)\n        overlap = len(predicted_k & actual_set)\n        recall_k = overlap / len(actual_set) * 100 if actual_set else 0\n        precision_k = overlap / k * 100\n        print(f\"  Top-{k}: {overlap}/{len(actual_set)} products, Recall: {recall_k:.1f}%, Precision: {precision_k:.1f}%\")\n\nprint(\"\\nTraining completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:47:19.786243Z","iopub.execute_input":"2025-11-18T11:47:19.786538Z","iopub.status.idle":"2025-11-18T11:54:07.978662Z","shell.execute_reply.started":"2025-11-18T11:47:19.786518Z","shell.execute_reply":"2025-11-18T11:54:07.977755Z"}},"outputs":[{"name":"stdout","text":"NEXT BASKET PREDICTION - FIXED MULTI-LABEL APPROACH\n============================================================\nUsing device: cuda\nWorking with 59,256 users\nCreated 59,256 sequences\nTrain: 41,479, Test: 17,777\n\nBUILDING COMPLETE VOCABULARY...\nComplete vocabulary: 22,554 products\n\nCHECKING VOCABULARY COVERAGE:\nVocabulary covers 59,256 out of 59,256 target products\nCoverage: 100.0%\nTraining samples: 41,479\n\nInitializing improved model...\nModel parameters: 15,652,890\n\nCALCULATING CLASS WEIGHTS...\nClass weights calculated for 22554 products\n\nStarting training...\nEpoch | Train Loss | Recall@10 | Precision@10 | F1@10\n-------------------------------------------------------\n    1 | 0.0115    | 7.35%     | 0.74%       | 1.34% | LR: 0.000997\n    2 | 0.0003    | 7.46%     | 0.75%       | 1.36% | LR: 0.000989\n    3 | 0.0003    | 7.03%     | 0.70%       | 1.28% | LR: 0.000976\n    4 | 0.0003    | 7.26%     | 0.73%       | 1.32% | LR: 0.000957\n    5 | 0.0003    | 6.87%     | 0.69%       | 1.25% | LR: 0.000933\n  -> Saved model with F1: 1.25%\n   10 | 0.0003    | 7.38%     | 0.74%       | 1.34% | LR: 0.000750\n  -> Saved model with F1: 1.34%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2753077561.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0mbest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/2753077561.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport time\n\nprint(\"NEXT BASKET PREDICTION - LIMITED VOCABULARY APPROACH\")\nprint(\"=\" * 60)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load and process data\ndf1 = pd.read_csv('/kaggle/input/simplifiedinstacartdata/all_order_products.csv')\ndf2 = pd.read_csv('/kaggle/input/simplifiedinstacartdata/orders.csv')\ndf = df2.merge(df1, on='order_id')\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf = df.sort_values(['user_id', 'timestamp', 'add_to_cart_order'])\n\n# Filter users with at least 4 orders\nuser_order_counts = df.groupby('user_id')['order_id'].nunique()\nvalid_users = user_order_counts[user_order_counts >= 4].index\ndf = df[df['user_id'].isin(valid_users)]\n\n# Add order sequence and keep first 4\ndf['order_seq'] = df.groupby('user_id').cumcount() + 1\ndf = df[df['order_seq'] <= 4]\n\n# Take only 30% of users\nall_users_with_4_orders = df['user_id'].unique()\nselected_users = np.random.choice(all_users_with_4_orders, \n                                 size=int(len(all_users_with_4_orders) * 0.3), \n                                 replace=False)\ndf = df[df['user_id'].isin(selected_users)]\n\nprint(f\"Working with {len(selected_users):,} users\")\n\n# Create sequences\nuser_sequences = {}\nfor user_id in df['user_id'].unique():\n    user_data = df[df['user_id'] == user_id]\n    orders = {}\n    for seq_num in range(1, 5):\n        order_products = user_data[user_data['order_seq'] == seq_num]['product_id'].tolist()\n        orders[seq_num] = order_products\n    if len(orders) == 4:\n        user_sequences[user_id] = {\n            'input': [orders[1], orders[2], orders[3]],\n            'target': orders[4]\n        }\n\nprint(f\"Created {len(user_sequences):,} sequences\")\n\n# Split users\nuser_ids = list(user_sequences.keys())\ntrain_users, test_users = train_test_split(user_ids, test_size=0.3, random_state=42)\ntrain_sequences = [user_sequences[uid] for uid in train_users]\ntest_sequences = [user_sequences[uid] for uid in test_users]\n\nprint(f\"Train: {len(train_sequences):,}, Test: {len(test_sequences):,}\")\n\n# ANALYZE BASKET SIZES\nprint(\"\\nANALYZING BASKET SIZES:\")\nprint(\"=\" * 50)\n\nbasket_sizes = []\nfor user_id in user_sequences:\n    for basket in user_sequences[user_id]['input']:\n        basket_sizes.append(len(basket))\n    basket_sizes.append(len(user_sequences[user_id]['target']))\n\nprint(f\"Total baskets analyzed: {len(basket_sizes):,}\")\nprint(f\"Min basket size: {min(basket_sizes)}\")\nprint(f\"Max basket size: {max(basket_sizes)}\")\nprint(f\"Average basket size: {np.mean(basket_sizes):.1f}\")\nprint(f\"Median basket size: {np.median(basket_sizes):.1f}\")\n\n# Calculate optimal max_basket_size (cover 95% of baskets)\nsorted_sizes = sorted(basket_sizes)\npercentile_95 = sorted_sizes[int(0.95 * len(sorted_sizes))]\npercentile_99 = sorted_sizes[int(0.99 * len(sorted_sizes))]\nprint(f\"95th percentile basket size: {percentile_95}\")\nprint(f\"99th percentile basket size: {percentile_99}\")\n\n# FIND TOP 500 PRODUCTS\nprint(\"\\nFINDING TOP 500 PRODUCTS:\")\nprint(\"=\" * 50)\n\n# Count product frequencies across ALL sequences (both input and target)\nproduct_counts = Counter()\nfor user_id in user_sequences:\n    # Count products in input baskets (orders 1-3)\n    for basket in user_sequences[user_id]['input']:\n        product_counts.update(basket)\n    # Count products in target basket (order 4)\n    product_counts.update(user_sequences[user_id]['target'])\n\nprint(f\"Total unique products: {len(product_counts):,}\")\n\n# Get top 500 products\ntop_500_products = [product for product, count in product_counts.most_common(500)]\ntop_500_counts = [count for product, count in product_counts.most_common(500)]\n\nprint(f\"\\nTop 10 products and their counts:\")\nfor i in range(10):\n    print(f\"  {i+1:2d}. Product {top_500_products[i]}: {top_500_counts[i]:,} occurrences\")\n\n# Calculate coverage statistics\ntotal_occurrences = sum(product_counts.values())\ntop_500_occurrences = sum(top_500_counts)\ncoverage = top_500_occurrences / total_occurrences * 100\n\nprint(f\"\\nCOVERAGE ANALYSIS:\")\nprint(f\"Total product occurrences: {total_occurrences:,}\")\nprint(f\"Top 500 products cover: {top_500_occurrences:,} occurrences\")\nprint(f\"Coverage: {coverage:.1f}% of all product occurrences\")\n\n# Check how many baskets contain at least one top-500 product\nbaskets_with_top_products = 0\ntotal_baskets = 0\n\nfor user_id in user_sequences:\n    # Check input baskets\n    for basket in user_sequences[user_id]['input']:\n        total_baskets += 1\n        if any(product in top_500_products for product in basket):\n            baskets_with_top_products += 1\n    \n    # Check target basket\n    total_baskets += 1\n    if any(product in top_500_products for product in user_sequences[user_id]['target']):\n        baskets_with_top_products += 1\n\nbasket_coverage = baskets_with_top_products / total_baskets * 100\nprint(f\"Baskets containing at least one top-500 product: {baskets_with_top_products:,}/{total_baskets:,} ({basket_coverage:.1f}%)\")\n\n# BUILD LIMITED VOCABULARY\nprint(\"\\nBUILDING LIMITED VOCABULARY:\")\nprint(\"=\" * 50)\n\n# Create vocabulary with only top 500 products + special tokens\nproduct_to_idx = {product: idx+1 for idx, product in enumerate(top_500_products)}\nidx_to_product = {idx: product for product, idx in product_to_idx.items()}\nvocab_size = len(product_to_idx) + 1  # +1 for padding (0)\n\nprint(f\"Limited vocabulary size: {vocab_size:,} products (500 + padding)\")\n\n# Check coverage in target baskets specifically\nprint(\"\\nTARGET BASKET COVERAGE ANALYSIS:\")\ntarget_coverage_stats = []\n\nfor seq_type, sequences in [(\"Train\", train_sequences), (\"Test\", test_sequences)]:\n    total_target_products = 0\n    covered_target_products = 0\n    baskets_with_covered_targets = 0\n    total_baskets = len(sequences)\n    \n    for seq in sequences:\n        target_products = seq['target']\n        total_target_products += len(target_products)\n        \n        covered_in_this_basket = 0\n        for product in target_products:\n            if product in product_to_idx:\n                covered_target_products += 1\n                covered_in_this_basket += 1\n        \n        if covered_in_this_basket > 0:\n            baskets_with_covered_targets += 1\n    \n    coverage_pct = covered_target_products / total_target_products * 100 if total_target_products > 0 else 0\n    basket_coverage_pct = baskets_with_covered_targets / total_baskets * 100\n    \n    target_coverage_stats.append((seq_type, coverage_pct, basket_coverage_pct))\n    \n    print(f\"{seq_type}:\")\n    print(f\"  Product coverage: {covered_target_products:,}/{total_target_products:,} ({coverage_pct:.1f}%)\")\n    print(f\"  Basket coverage: {baskets_with_covered_targets:,}/{total_baskets:,} ({basket_coverage_pct:.1f}%)\")\n\n# Dataset with limited vocabulary\nclass LimitedVocabDataset(Dataset):\n    def __init__(self, sequences, product_to_idx, max_basket_size=20):\n        self.sequences = sequences\n        self.product_to_idx = product_to_idx\n        self.max_basket_size = max_basket_size\n        self.vocab_size = len(product_to_idx) + 1\n        \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        sequence = self.sequences[idx]\n        \n        # Input: first 3 orders as baskets\n        input_orders = []\n        for basket in sequence['input']:\n            basket_idx = []\n            for p in basket[:self.max_basket_size]:\n                if p in self.product_to_idx:\n                    basket_idx.append(self.product_to_idx[p])\n            # Pad basket to max_basket_size\n            padded_basket = basket_idx + [0] * (self.max_basket_size - len(basket_idx))\n            input_orders.append(padded_basket)\n        \n        input_tensor = torch.tensor(input_orders, dtype=torch.long)\n        \n        # Target: 4th order as multi-hot encoding (only for top 500 products)\n        target = torch.zeros(self.vocab_size)\n        for product in sequence['target']:\n            if product in self.product_to_idx:\n                target[self.product_to_idx[product]] = 1.0\n        \n        return input_tensor, target\n\n# Use 95th percentile for basket size to cover most baskets efficiently\nmax_basket_size = min(percentile_95, 30)  # Cap at 30 to avoid extreme values\nprint(f\"\\nUsing max_basket_size: {max_basket_size}\")\n\n# Create datasets with limited vocabulary\ntrain_dataset = LimitedVocabDataset(train_sequences, product_to_idx, max_basket_size)\ntest_dataset = LimitedVocabDataset(test_sequences, product_to_idx, max_basket_size)\n\nprint(f\"Training samples: {len(train_dataset):,}\")\n\n# Calculate new class imbalance\nprint(\"\\nNEW CLASS IMBALANCE ANALYSIS:\")\npositive_count = 0\ntotal_count = 0\n\nfor _, target in train_dataset:\n    positive_count += target.sum().item()\n    total_count += target.numel()\n\npositive_ratio = positive_count / total_count\nnegative_ratio = 1 - positive_ratio\n\nprint(f\"Positive ratio: {positive_ratio:.6f}\")\nprint(f\"Negative ratio: {negative_ratio:.6f}\")\nprint(f\"Imbalance ratio: 1:{int(negative_ratio/positive_ratio):,}\")\n\n# Simple model for limited vocabulary\nclass LimitedVocabModel(nn.Module):\n    def __init__(self, vocab_size, d_model=128, max_basket_size=20):\n        super(LimitedVocabModel, self).__init__()\n        \n        self.product_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n        \n        self.encoder = nn.Sequential(\n            nn.Linear(3 * max_basket_size * d_model, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, vocab_size)\n        )\n        \n    def forward(self, x):\n        batch_size = x.size(0)\n        embedded = self.product_embedding(x)\n        flattened = embedded.view(batch_size, -1)\n        output = self.encoder(flattened)\n        return output\n\nprint(\"\\nInitializing limited vocabulary model...\")\nmodel = LimitedVocabModel(vocab_size, max_basket_size=max_basket_size)\nmodel.to(device)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Use proper class weighting for the new imbalance\npos_weight = min(negative_ratio / positive_ratio, 100.0)  # Cap at 100x\nprint(f\"Using pos_weight: {pos_weight:.1f}\")\n\ncriterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Training and evaluation functions (same as before)\ndef calculate_basket_accuracy(model, loader, k=10):\n    model.eval()\n    total_recall = 0\n    total_precision = 0\n    total_f1 = 0\n    num_baskets = 0\n    \n    with torch.no_grad():\n        for data, target in loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            probabilities = torch.sigmoid(output)\n            \n            _, topk_indices = torch.topk(probabilities, k, dim=1)\n            \n            for i in range(len(target)):\n                actual_indices = set((target[i] == 1).nonzero(as_tuple=True)[0].cpu().numpy())\n                \n                if len(actual_indices) == 0:\n                    continue\n                \n                predicted_indices = set(topk_indices[i].cpu().numpy())\n                \n                recall = len(actual_indices & predicted_indices) / len(actual_indices)\n                precision = len(actual_indices & predicted_indices) / k\n                \n                total_recall += recall\n                total_precision += precision\n                \n                if recall + precision > 0:\n                    f1 = 2 * (recall * precision) / (recall + precision)\n                    total_f1 += f1\n                \n                num_baskets += 1\n    \n    if num_baskets == 0:\n        return 0, 0, 0\n    \n    avg_recall = total_recall / num_baskets * 100\n    avg_precision = total_precision / num_baskets * 100  \n    avg_f1 = total_f1 / num_baskets * 100\n    \n    return avg_recall, avg_precision, avg_f1\n\ndef train_epoch():\n    model.train()\n    total_loss = 0\n    for data, target in train_loader:\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(train_loader)\n\nprint(\"\\nStarting training with limited vocabulary...\")\nprint(\"Epoch | Train Loss | Recall@10 | Precision@10 | F1@10\")\nprint(\"-\" * 55)\n\nbest_f1 = 0\nfor epoch in range(1, 51):\n    train_loss = train_epoch()\n    \n    if epoch % 5 == 0 or epoch <= 5:\n        recall, precision, f1 = calculate_basket_accuracy(model, test_loader, k=10)\n        print(f\"{epoch:5d} | {train_loss:.4f}    | {recall:.2f}%     | {precision:.2f}%       | {f1:.2f}%\")\n        \n        if f1 > best_f1 and epoch >= 3:\n            best_f1 = f1\n            torch.save(model.state_dict(), 'best_limited_vocab_model.pth')\n            print(f\"  -> Saved model with F1: {f1:.2f}%\")\n\nprint(f\"\\nBest F1 score: {best_f1:.2f}%\")\n\n# Final evaluation\nprint(\"\\nFINAL EVALUATION:\")\nmodel.load_state_dict(torch.load('best_limited_vocab_model.pth'))\nfor k in [5, 10, 15, 20]:\n    recall, precision, f1 = calculate_basket_accuracy(model, test_loader, k=k)\n    print(f\"Top-{k:2d}: Recall: {recall:.2f}% | Precision: {precision:.2f}% | F1: {f1:.2f}%\")\n\nprint(\"\\nTraining completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T12:07:13.029766Z","iopub.execute_input":"2025-11-18T12:07:13.030591Z","iopub.status.idle":"2025-11-18T12:13:18.466199Z","shell.execute_reply.started":"2025-11-18T12:07:13.030564Z","shell.execute_reply":"2025-11-18T12:13:18.465398Z"}},"outputs":[{"name":"stdout","text":"NEXT BASKET PREDICTION - LIMITED VOCABULARY APPROACH\n============================================================\nUsing device: cuda\nWorking with 59,256 users\nCreated 59,256 sequences\nTrain: 41,479, Test: 17,777\n\nANALYZING BASKET SIZES:\n==================================================\nTotal baskets analyzed: 237,024\nMin basket size: 1\nMax basket size: 1\nAverage basket size: 1.0\nMedian basket size: 1.0\n95th percentile basket size: 1\n99th percentile basket size: 1\n\nFINDING TOP 500 PRODUCTS:\n==================================================\nTotal unique products: 22,412\n\nTop 10 products and their counts:\n   1. Product 24852: 5,579 occurrences\n   2. Product 13176: 3,698 occurrences\n   3. Product 21137: 2,411 occurrences\n   4. Product 47766: 2,201 occurrences\n   5. Product 21903: 1,953 occurrences\n   6. Product 47209: 1,654 occurrences\n   7. Product 27845: 1,305 occurrences\n   8. Product 16797: 1,289 occurrences\n   9. Product 47626: 982 occurrences\n  10. Product 26209: 907 occurrences\n\nCOVERAGE ANALYSIS:\nTotal product occurrences: 237,024\nTop 500 products cover: 107,442 occurrences\nCoverage: 45.3% of all product occurrences\nBaskets containing at least one top-500 product: 107,442/237,024 (45.3%)\n\nBUILDING LIMITED VOCABULARY:\n==================================================\nLimited vocabulary size: 501 products (500 + padding)\n\nTARGET BASKET COVERAGE ANALYSIS:\nTrain:\n  Product coverage: 18,017/41,479 (43.4%)\n  Basket coverage: 18,017/41,479 (43.4%)\nTest:\n  Product coverage: 7,791/17,777 (43.8%)\n  Basket coverage: 7,791/17,777 (43.8%)\n\nUsing max_basket_size: 1\nTraining samples: 41,479\n\nNEW CLASS IMBALANCE ANALYSIS:\nPositive ratio: 0.000867\nNegative ratio: 0.999133\nImbalance ratio: 1:1,152\n\nInitializing limited vocabulary model...\nModel parameters: 521,333\nUsing pos_weight: 100.0\n\nStarting training with limited vocabulary...\nEpoch | Train Loss | Recall@10 | Precision@10 | F1@10\n-------------------------------------------------------\n    1 | 0.2890    | 19.25%     | 1.93%       | 3.50%\n    2 | 0.2605    | 21.04%     | 2.10%       | 3.82%\n    3 | 0.2429    | 21.72%     | 2.17%       | 3.95%\n  -> Saved model with F1: 3.95%\n    4 | 0.2284    | 21.02%     | 2.10%       | 3.82%\n    5 | 0.2145    | 20.29%     | 2.03%       | 3.69%\n   10 | 0.1705    | 18.29%     | 1.83%       | 3.33%\n   15 | 0.1557    | 17.39%     | 1.74%       | 3.16%\n   20 | 0.1513    | 16.92%     | 1.69%       | 3.08%\n   25 | 0.1528    | 16.89%     | 1.69%       | 3.07%\n   30 | 0.1550    | 16.92%     | 1.69%       | 3.08%\n   35 | 0.1568    | 17.02%     | 1.70%       | 3.09%\n   40 | 0.1635    | 16.93%     | 1.69%       | 3.08%\n   45 | 0.1680    | 17.07%     | 1.71%       | 3.10%\n   50 | 0.1725    | 17.49%     | 1.75%       | 3.18%\n\nBest F1 score: 3.95%\n\nFINAL EVALUATION:\nTop- 5: Recall: 14.93% | Precision: 2.99% | F1: 4.98%\nTop-10: Recall: 21.72% | Precision: 2.17% | F1: 3.95%\nTop-15: Recall: 25.89% | Precision: 1.73% | F1: 3.24%\nTop-20: Recall: 29.29% | Precision: 1.46% | F1: 2.79%\n\nTraining completed!\n","output_type":"stream"}],"execution_count":24}]}
